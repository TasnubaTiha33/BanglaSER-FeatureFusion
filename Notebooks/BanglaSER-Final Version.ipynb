{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# üîÅ Full Reproducibility + Warning Suppression Setup\n",
    "# ==================================================\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# === ENVIRONMENT VARIABLES (SET BEFORE TF IMPORT) ===\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)           # Hash seed\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'           # Suppress TensorFlow logs\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '0'           # Allow non-deterministic ops to avoid UnimplementedError\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'           # Set GPU ID (or \"\" to force CPU)\n",
    "\n",
    "# === PYTHON SEED SETTINGS ===\n",
    "random.seed(SEED)\n",
    "\n",
    "# === SUPPRESS WARNINGS & LOGGING ===\n",
    "logging.getLogger('absl').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# === IMPORT LIBRARIES AFTER SEED SETTINGS ===\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === NUMPY & TENSORFLOW SEEDS ===\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# === SINGLE-THREADING FOR FULL REPRODUCIBILITY ===\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "# ‚úÖ CHECK GPU AVAILABILITY\n",
    "print(\"‚úÖ GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# ‚úÖ DATASET PATH\n",
    "DATASET_PATH = r'D:\\498R\\BanglaSER'\n",
    "\n",
    "# ‚úÖ EMOTION LABELS FROM FILENAME\n",
    "# Format: Mode-StatementType-Emotion-Intensity-Statement-Repetition-Actor.wav\n",
    "EMOTION_MAPPING = {\n",
    "    '01': 'happy',\n",
    "    '02': 'sad',\n",
    "    '03': 'angry',\n",
    "    '04': 'surprise',\n",
    "    '05': 'neutral'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset information from file structure\n",
    "def create_dataset_info(root_path):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                # Parse the filename to get emotion\n",
    "                # Format: Mode-StatementType-Emotion-Intensity-Statement-Repetition-Actor.wav\n",
    "                parts = file.split('-')\n",
    "                if len(parts) == 7:\n",
    "                    emotion_code = parts[2]\n",
    "                    emotion = EMOTION_MAPPING.get(emotion_code, 'unknown')\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    data.append({\n",
    "                        'file_path': file_path,\n",
    "                        'emotion': emotion,\n",
    "                        'emotion_code': emotion_code\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create and display dataset info\n",
    "df = create_dataset_info(DATASET_PATH)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(df['emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get class distribution\n",
    "emotion_counts = df['emotion'].value_counts()\n",
    "\n",
    "# Pie chart\n",
    "fig, ax = plt.subplots(figsize=(8,8), facecolor='none')  # transparent figure background\n",
    "ax.set_facecolor('none')  # transparent axes background\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    emotion_counts, \n",
    "    labels=emotion_counts.index, \n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90, \n",
    "    colors=plt.cm.Set3.colors,\n",
    "    textprops={'fontsize': 19, 'fontweight': 'bold'}\n",
    ")\n",
    "\n",
    "# Bolden the percentages\n",
    "for autotext in autotexts:\n",
    "    autotext.set_fontsize(19)\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "# ---------------------------\n",
    "# Audio loading & utilities\n",
    "# ---------------------------\n",
    "TARGET_SR = 16000\n",
    "DURATION_SEC = 4.0\n",
    "TARGET_LEN = int(TARGET_SR * DURATION_SEC)\n",
    "\n",
    "def load_audio_fixed(file_path, sr=TARGET_SR, duration=DURATION_SEC):\n",
    "    audio, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
    "    if len(audio) < TARGET_LEN:\n",
    "        audio = np.pad(audio, (0, TARGET_LEN - len(audio)), mode='constant')\n",
    "    else:\n",
    "        audio = audio[:TARGET_LEN]\n",
    "    return audio, sr\n",
    "\n",
    "def ensure_length(audio, target_len=TARGET_LEN):\n",
    "    if len(audio) < target_len:\n",
    "        audio = np.pad(audio, (0, target_len - len(audio)), mode='constant')\n",
    "    elif len(audio) > target_len:\n",
    "        audio = audio[:target_len]\n",
    "    return audio\n",
    "\n",
    "# ---------------------------\n",
    "# Audio-domain augmentations\n",
    "# ---------------------------\n",
    "def aug_noise(audio, noise_std=0.005):\n",
    "    return audio + np.random.normal(0.0, noise_std, size=audio.shape)\n",
    "\n",
    "def aug_shift(audio, max_shift_ratio=0.1):\n",
    "    \"\"\"Zero-pad shift (no wrap-around).\"\"\"\n",
    "    max_shift = int(len(audio) * max_shift_ratio)\n",
    "    shift = np.random.randint(-max_shift, max_shift + 1)\n",
    "    if shift == 0:\n",
    "        return audio\n",
    "    if shift > 0:\n",
    "        # delay: pad at start, drop end\n",
    "        return np.concatenate([np.zeros(shift, dtype=audio.dtype), audio[:-shift]])\n",
    "    else:\n",
    "        # advance: drop start, pad at end\n",
    "        s = -shift\n",
    "        return np.concatenate([audio[s:], np.zeros(s, dtype=audio.dtype)])\n",
    "\n",
    "def aug_pitch(audio, sr=TARGET_SR, semitone_range=(-2, 2)):\n",
    "    steps = np.random.uniform(semitone_range[0], semitone_range[1])\n",
    "    y = librosa.effects.pitch_shift(y=audio, sr=sr, n_steps=steps)\n",
    "    return ensure_length(y, TARGET_LEN)\n",
    "\n",
    "def aug_stretch(audio, rate_range=(0.8, 1.25)):\n",
    "    rate = np.random.uniform(rate_range[0], rate_range[1])\n",
    "    # In some librosa versions, 'rate' is keyword-only\n",
    "    y = librosa.effects.time_stretch(y=audio, rate=rate)\n",
    "    return ensure_length(y, TARGET_LEN)\n",
    "\n",
    "AUG_FUNCS = ['noise', 'shift', 'pitch', 'stretch']\n",
    "\n",
    "def apply_aug(audio, method):\n",
    "    if method == 'noise':\n",
    "        return aug_noise(audio)\n",
    "    elif method == 'shift':\n",
    "        return aug_shift(audio)\n",
    "    elif method == 'pitch':\n",
    "        return aug_pitch(audio)\n",
    "    elif method == 'stretch':\n",
    "        return aug_stretch(audio)\n",
    "    return audio\n",
    "\n",
    "# ---------------------------\n",
    "# Feature extraction (unchanged logic, but from audio)\n",
    "# ---------------------------\n",
    "def extract_features_from_audio(audio, sr=TARGET_SR):\n",
    "    mfccs   = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    mel_s   = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)\n",
    "    log_mel = librosa.power_to_db(mel_s)\n",
    "    zcr     = librosa.feature.zero_crossing_rate(audio)\n",
    "    chroma  = librosa.feature.chroma_stft(y=audio, sr=sr, n_chroma=12)\n",
    "    rms     = librosa.feature.rms(y=audio)\n",
    "\n",
    "    features = np.vstack([mfccs, log_mel, zcr, chroma, rms])  # (155, T)\n",
    "    return features.T  # (T, 155)\n",
    "\n",
    "# If you need the same signature as before:\n",
    "def extract_features(file_path):\n",
    "    audio, sr = load_audio_fixed(file_path, sr=TARGET_SR, duration=DURATION_SEC)\n",
    "    return extract_features_from_audio(audio, sr)\n",
    "\n",
    "# ---------------------------\n",
    "# Label mapping (same as yours)\n",
    "# ---------------------------\n",
    "emotions = df['emotion'].unique()\n",
    "label_mapping = {label: i for i, label in enumerate(emotions)}\n",
    "reverse_mapping = {i: label for label, i in label_mapping.items()}\n",
    "num_labels = len(emotions)\n",
    "\n",
    "print(\"\\nEmotion Label Mapping:\")\n",
    "for emotion, idx in label_mapping.items():\n",
    "    print(f\"{emotion}: {idx}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# PREPROCESS + AUDIO AUGMENTATION (before split)\n",
    "# ---------------------------------------------\n",
    "print(\"üîÅ Extracting ORIGINAL features for ALL files ...\")\n",
    "X_all, y_all = [], []\n",
    "audios_by_class = defaultdict(list)\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"All data\"):\n",
    "    audio, sr = load_audio_fixed(row['file_path'], sr=TARGET_SR, duration=DURATION_SEC)\n",
    "    # keep raw audio per class for later augmentation\n",
    "    label = label_mapping[row['emotion']]\n",
    "    audios_by_class[label].append(audio)\n",
    "\n",
    "    # original features\n",
    "    feats = extract_features_from_audio(audio, sr)\n",
    "    X_all.append(feats)\n",
    "    y_all.append(label)\n",
    "\n",
    "print(f\"üìä Original samples: {len(X_all)}\")\n",
    "\n",
    "# Class-balanced augmentation over WHOLE set (audio-domain)\n",
    "target_per_class = 1000  # your target\n",
    "aug_X_all, aug_y_all = [], []\n",
    "\n",
    "print(\"üéõÔ∏è Augmenting (audio-domain) to balance classes ...\")\n",
    "for label, audios in audios_by_class.items():\n",
    "    count = len(audios)\n",
    "    needed = max(0, target_per_class - count)\n",
    "    i = 0\n",
    "    while needed > 0:\n",
    "        base = audios[i % count]\n",
    "        method = AUG_FUNCS[np.random.randint(0, len(AUG_FUNCS))]\n",
    "        aug_audio = apply_aug(base, method)\n",
    "        aug_audio = ensure_length(aug_audio, TARGET_LEN)  # safety\n",
    "        feats = extract_features_from_audio(aug_audio, TARGET_SR)\n",
    "        aug_X_all.append(feats)\n",
    "        aug_y_all.append(label)\n",
    "        i += 1\n",
    "        needed -= 1\n",
    "\n",
    "# Combine originals + augmented (BEFORE split)\n",
    "X_all = np.concatenate([np.array(X_all), np.array(aug_X_all)], axis=0)\n",
    "y_all = np.concatenate([np.array(y_all), np.array(aug_y_all)], axis=0)\n",
    "\n",
    "print(f\"‚úÖ Final samples after class-balanced augmentation: {X_all.shape[0]} (shape per sample: {X_all.shape[1:]} )\")\n",
    "\n",
    "# -----------------\n",
    "# Split (as you want)\n",
    "# -----------------\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train)} | Val set: {len(X_val)} | Test set: {len(X_test)}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Feature scaling (fit on TRAIN only)\n",
    "# ---------------------------------------\n",
    "# Flatten across time to fit a scaler on feature dims\n",
    "T, F = X_train.shape[1], X_train.shape[2]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.reshape(-1, F))  # train-only fit\n",
    "\n",
    "def transform_with_scaler(X, scaler):\n",
    "    N, T, F = X.shape\n",
    "    X2 = X.reshape(-1, F)\n",
    "    X2 = scaler.transform(X2)\n",
    "    return X2.reshape(N, T, F)\n",
    "\n",
    "X_train = transform_with_scaler(X_train, scaler)\n",
    "X_val   = transform_with_scaler(X_val, scaler)\n",
    "X_test  = transform_with_scaler(X_test, scaler)\n",
    "\n",
    "# -------------------------\n",
    "# Build TensorFlow datasets\n",
    "# -------------------------\n",
    "batch_size = 32\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds   = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds  = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"\\n‚úÖ Dataset preparation complete (AUGMENT ‚Üí SPLIT).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Pick one sample (e.g., first training sample)\n",
    "idx = 0  \n",
    "audio = X_train[idx].reshape(-1)  \n",
    "\n",
    "# Extract individual features\n",
    "mfccs   = librosa.feature.mfcc(y=audio, sr=TARGET_SR, n_mfcc=13)\n",
    "mel_s   = librosa.feature.melspectrogram(y=audio, sr=TARGET_SR, n_mels=128)\n",
    "log_mel = librosa.power_to_db(mel_s)\n",
    "zcr     = librosa.feature.zero_crossing_rate(audio)\n",
    "chroma  = librosa.feature.chroma_stft(y=audio, sr=TARGET_SR, n_chroma=12)\n",
    "rms     = librosa.feature.rms(y=audio)\n",
    "\n",
    "features_dict = {\n",
    "    \"MFCCs\": (mfccs, \"magma\"),\n",
    "    \"Log-Mel\": (log_mel, \"inferno\"),\n",
    "    \"ZCR\": (zcr, \"plasma\"),\n",
    "    \"Chroma\": (chroma, \"cividis\"),\n",
    "    \"RMS\": (rms, \"coolwarm\")\n",
    "}\n",
    "\n",
    "# Folder to save images\n",
    "os.makedirs(\"feature_heatmaps_v2\", exist_ok=True)\n",
    "\n",
    "# Save each feature as a separate heatmap with new color + name\n",
    "for name, (feat, cmap) in features_dict.items():\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(feat, aspect='auto', origin='lower', cmap=cmap)\n",
    "    plt.title(f\"{name}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.colorbar(format=\"%+2.1f dB\" if name==\"Log-Mel\" else None)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        f\"feature_heatmaps_v2/{name}_v2.png\",  # different filenames\n",
    "        transparent=True,\n",
    "        dpi=300\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "print(\"‚úÖ Saved 5 new heatmaps with different colors in 'feature_heatmaps_v2/' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example: your real counts\n",
    "before_counts = [len(audios) for label, audios in audios_by_class.items()]\n",
    "after_counts  = [1000 for _ in audios_by_class]   # since you balanced to 1000\n",
    "class_labels  = [reverse_mapping[label] for label in audios_by_class.keys()]\n",
    "\n",
    "x = np.arange(len(class_labels))\n",
    "width = 0.5  # üîπ Increased thickness\n",
    "\n",
    "# üîπ Transparent figure + axes\n",
    "fig, ax = plt.subplots(figsize=(10,6), facecolor='none')\n",
    "ax.set_facecolor('none')\n",
    "\n",
    "# Plot bars\n",
    "rects1 = ax.bar(x - width/2, before_counts, width, \n",
    "                label=\"Before Augmentation\", color='skyblue')\n",
    "rects2 = ax.bar(x + width/2, after_counts, width, \n",
    "                label=\"After Augmentation\", color='lightcoral')\n",
    "\n",
    "# Titles and labels\n",
    "ax.set_ylabel(\"Number of Samples\", fontsize=16, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_labels, rotation=0, fontsize=14, fontweight='bold')  # üîπ No tilt\n",
    "ax.tick_params(axis='y', labelsize=14, width=2)\n",
    "\n",
    "# Add value labels above bars\n",
    "def autolabel(rects, color):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 5),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=14, fontweight='bold', color=color)\n",
    "\n",
    "autolabel(rects1, 'black')\n",
    "autolabel(rects2, 'black')\n",
    "\n",
    "# Legend\n",
    "ax.legend(\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 1),\n",
    "    borderaxespad=0,\n",
    "    frameon=False,\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "# üîπ Remove outer box (all spines)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show transparent plot\n",
    "plt.show()\n",
    "\n",
    "# üîπ If saving, keep transparency\n",
    "# plt.savefig(\"augmentation_counts.png\", dpi=300, transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape =X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Duplicate detection/cleaning + GRAPHS\n",
    "# ============================\n",
    "import hashlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# --- snapshot \"before\" for graphs ---\n",
    "X_train_bef, y_train_bef = X_train.copy(), y_train.copy()\n",
    "X_val_bef,   y_val_bef   = X_val.copy(),   y_val.copy()\n",
    "X_test_bef,  y_test_bef  = X_test.copy(),  y_test.copy()\n",
    "\n",
    "def hash_sample(arr, decimals=6):\n",
    "    \"\"\"Stable hash for a (T,F) float array after rounding (avoids tiny float jitter).\"\"\"\n",
    "    a = np.ascontiguousarray(np.round(arr, decimals))\n",
    "    return hashlib.sha1(a.view(np.uint8)).hexdigest()\n",
    "\n",
    "def hashes_for_split(X):\n",
    "    return np.array([hash_sample(x) for x in X])\n",
    "\n",
    "def find_within_split_dups(hashes):\n",
    "    buckets = defaultdict(list)\n",
    "    for i, h in enumerate(hashes):\n",
    "        buckets[h].append(i)\n",
    "    # groups with more than 1 index\n",
    "    return {h: idxs for h, idxs in buckets.items() if len(idxs) > 1}\n",
    "\n",
    "def count_within_extra(dups_dict):\n",
    "    # number of EXTRA items (i.e., total dup count minus 1 per group)\n",
    "    return sum(len(idxs) - 1 for idxs in dups_dict.values())\n",
    "\n",
    "def keep_first_indices(hashes):\n",
    "    \"\"\"Indices that keep the first appearance of each hash (deduplicate in-place).\"\"\"\n",
    "    seen, keep = set(), []\n",
    "    for i, h in enumerate(hashes):\n",
    "        if h not in seen:\n",
    "            seen.add(h)\n",
    "            keep.append(i)\n",
    "    return np.array(keep, dtype=int)\n",
    "\n",
    "def apply_mask(X, y, keep_idx):\n",
    "    mask = np.zeros(len(X), dtype=bool)\n",
    "    mask[keep_idx] = True\n",
    "    return X[mask], y[mask], mask\n",
    "\n",
    "# --- compute hashes BEFORE cleaning ---\n",
    "train_h_bef = hashes_for_split(X_train_bef)\n",
    "val_h_bef   = hashes_for_split(X_val_bef)\n",
    "test_h_bef  = hashes_for_split(X_test_bef)\n",
    "\n",
    "dups_train_bef = find_within_split_dups(train_h_bef)\n",
    "dups_val_bef   = find_within_split_dups(val_h_bef)\n",
    "dups_test_bef  = find_within_split_dups(test_h_bef)\n",
    "\n",
    "within_train_bef = count_within_extra(dups_train_bef)\n",
    "within_val_bef   = count_within_extra(dups_val_bef)\n",
    "within_test_bef  = count_within_extra(dups_test_bef)\n",
    "\n",
    "# cross-split overlaps BEFORE cleaning\n",
    "train_set_bef = set(train_h_bef)\n",
    "val_set_bef   = set(val_h_bef)\n",
    "test_set_bef  = set(test_h_bef)\n",
    "\n",
    "val_in_train_bef  = np.where(np.isin(val_h_bef,  list(train_set_bef)))[0]\n",
    "test_in_train_bef = np.where(np.isin(test_h_bef, list(train_set_bef)))[0]\n",
    "train_in_val_bef  = np.where(np.isin(train_h_bef, list(val_set_bef)))[0]\n",
    "train_in_test_bef = np.where(np.isin(train_h_bef, list(test_set_bef)))[0]\n",
    "val_in_test_bef   = np.where(np.isin(val_h_bef,  list(test_set_bef)))[0]\n",
    "test_in_val_bef   = np.where(np.isin(test_h_bef, list(val_set_bef)))[0]\n",
    "\n",
    "print(\"\\n=== DUPLICATE SUMMARY (before cleaning) ===\")\n",
    "print(f\"[TRAIN] within groups: {len(dups_train_bef)} | extras: {within_train_bef}\")\n",
    "print(f\"[VAL]   within groups: {len(dups_val_bef)}   | extras: {within_val_bef}\")\n",
    "print(f\"[TEST]  within groups: {len(dups_test_bef)}  | extras: {within_test_bef}\")\n",
    "print(f\"[VAL]   duplicates in TRAIN: {len(val_in_train_bef)}\")\n",
    "print(f\"[TEST]  duplicates in TRAIN: {len(test_in_train_bef)}\")\n",
    "print(f\"[TRAIN] duplicates in VAL:   {len(train_in_val_bef)}\")\n",
    "print(f\"[TRAIN] duplicates in TEST:  {len(train_in_test_bef)}\")\n",
    "print(f\"[VAL]   duplicates in TEST:  {len(val_in_test_bef)}\")\n",
    "print(f\"[TEST]  duplicates in VAL:   {len(test_in_val_bef)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Remove duplicates (toggle)\n",
    "# -----------------------------\n",
    "REMOVE_WITHIN_SPLIT = True          # drop duplicates within each split (keep first)\n",
    "REMOVE_VALTEST_IF_IN_TRAIN = True   # drop val/test items that also appear in train\n",
    "\n",
    "changed = False\n",
    "\n",
    "# 1) within-split dedupe\n",
    "if REMOVE_WITHIN_SPLIT:\n",
    "    keep_tr = keep_first_indices(train_h_bef)\n",
    "    keep_va = keep_first_indices(val_h_bef)\n",
    "    keep_te = keep_first_indices(test_h_bef)\n",
    "\n",
    "    if len(keep_tr) < len(X_train) or len(keep_va) < len(X_val) or len(keep_te) < len(X_test):\n",
    "        X_train, y_train, _ = apply_mask(X_train, y_train, keep_tr)\n",
    "        X_val,   y_val,   _ = apply_mask(X_val,   y_val,   keep_va)\n",
    "        X_test,  y_test,  _ = apply_mask(X_test,  y_test,  keep_te)\n",
    "        changed = True\n",
    "\n",
    "# re-hash after within-split\n",
    "train_hashes = hashes_for_split(X_train)\n",
    "val_hashes   = hashes_for_split(X_val)\n",
    "test_hashes  = hashes_for_split(X_test)\n",
    "train_set    = set(train_hashes)\n",
    "\n",
    "# 2) remove any VAL/TEST samples that appear in TRAIN\n",
    "if REMOVE_VALTEST_IF_IN_TRAIN:\n",
    "    val_keep_idx  = np.where(~np.isin(val_hashes,  list(train_set)))[0]\n",
    "    test_keep_idx = np.where(~np.isin(test_hashes, list(train_set)))[0]\n",
    "    if len(val_keep_idx) < len(X_val) or len(test_keep_idx) < len(X_test):\n",
    "        X_val,  y_val,  _ = apply_mask(X_val,  y_val,  val_keep_idx)\n",
    "        X_test, y_test, _ = apply_mask(X_test, y_test, test_keep_idx)\n",
    "        changed = True\n",
    "\n",
    "print(\"\\n\" + (\"Duplicates removed.\" if changed else \"No duplicates removed.\"))\n",
    "print(f\"Sizes ‚Äî Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "# --- POST-clean for graphs ---\n",
    "train_h_aft = hashes_for_split(X_train)\n",
    "val_h_aft   = hashes_for_split(X_val)\n",
    "test_h_aft  = hashes_for_split(X_test)\n",
    "\n",
    "dups_train_aft = find_within_split_dups(train_h_aft)\n",
    "dups_val_aft   = find_within_split_dups(val_h_aft)\n",
    "dups_test_aft  = find_within_split_dups(test_h_aft)\n",
    "\n",
    "within_train_aft = count_within_extra(dups_train_aft)\n",
    "within_val_aft   = count_within_extra(dups_val_aft)\n",
    "within_test_aft  = count_within_extra(dups_test_aft)\n",
    "\n",
    "# ==================\n",
    "# GRAPHS\n",
    "# ==================\n",
    "\n",
    "# 1) Counts before vs after\n",
    "sizes_before = [len(X_train_bef), len(X_val_bef), len(X_test_bef)]\n",
    "sizes_after  = [len(X_train),     len(X_val),     len(X_test)]\n",
    "splits = ['Train', 'Val', 'Test']\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "x = np.arange(len(splits))\n",
    "w = 0.38\n",
    "plt.bar(x - w/2, sizes_before, width=w, label='Before')\n",
    "plt.bar(x + w/2, sizes_after,  width=w, label='After')\n",
    "plt.xticks(x, splits)\n",
    "plt.ylabel('Num samples')\n",
    "plt.title('Split sizes: before vs after duplicate cleaning')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('dup_sizes_before_after.png')\n",
    "plt.close()\n",
    "\n",
    "# 2) Within-split duplicates (extras) before vs after\n",
    "within_bef = [within_train_bef, within_val_bef, within_test_bef]\n",
    "within_aft = [within_train_aft, within_val_aft, within_test_aft]\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(x - w/2, within_bef, width=w, label='Before')\n",
    "plt.bar(x + w/2, within_aft, width=w, label='After')\n",
    "plt.xticks(x, splits)\n",
    "plt.ylabel('Num duplicate extras')\n",
    "plt.title('Within-split duplicate extras: before vs after')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('dup_within_extras_before_after.png')\n",
    "plt.close()\n",
    "\n",
    "# 3) Cross-split overlap heatmap (BEFORE cleaning)\n",
    "overlap_mat = np.array([\n",
    "    [within_train_bef, len(train_in_val_bef), len(train_in_test_bef)],\n",
    "    [len(val_in_train_bef), within_val_bef,   len(val_in_test_bef)],\n",
    "    [len(test_in_train_bef), len(test_in_val_bef), within_test_bef]\n",
    "], dtype=int)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.heatmap(overlap_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Train','Val','Test'],\n",
    "            yticklabels=['Train','Val','Test'])\n",
    "plt.title('Duplicate overlap (BEFORE cleaning)\\n(diagonal = within extras)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('dup_overlap_heatmap_before.png')\n",
    "plt.close()\n",
    "\n",
    "# 4) Class distribution per split ‚Äî before and after (stacked bars)\n",
    "def plot_class_dist(y_train, y_val, y_test, title, fname):\n",
    "    classes = [reverse_mapping[i] for i in range(len(reverse_mapping))]\n",
    "    # counts per class per split\n",
    "    counts_tr = np.array([np.sum(y_train == i) for i in range(len(reverse_mapping))])\n",
    "    counts_va = np.array([np.sum(y_val   == i) for i in range(len(reverse_mapping))])\n",
    "    counts_te = np.array([np.sum(y_test  == i) for i in range(len(reverse_mapping))])\n",
    "\n",
    "    # stacked bars across splits\n",
    "    plt.figure(figsize=(10,6))\n",
    "    bottoms = np.zeros(3)\n",
    "    for i, cls in enumerate(classes):\n",
    "        vals = np.array([counts_tr[i], counts_va[i], counts_te[i]])\n",
    "        plt.bar(splits, vals, bottom=bottoms, label=cls)\n",
    "        bottoms += vals\n",
    "    plt.ylabel('Num samples')\n",
    "    plt.title(title)\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname)\n",
    "    plt.close()\n",
    "\n",
    "plot_class_dist(y_train_bef, y_val_bef, y_test_bef,\n",
    "                'Class distribution per split (BEFORE cleaning)',\n",
    "                'class_dist_before.png')\n",
    "\n",
    "plot_class_dist(y_train, y_val, y_test,\n",
    "                'Class distribution per split (AFTER cleaning)',\n",
    "                'class_dist_after.png')\n",
    "\n",
    "print(\"üìà Saved figures:\",\n",
    "      \"dup_sizes_before_after.png,\",\n",
    "      \"dup_within_extras_before_after.png,\",\n",
    "      \"dup_overlap_heatmap_before.png,\",\n",
    "      \"class_dist_before.png,\",\n",
    "      \"class_dist_after.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# ===============================\n",
    "# üîß Configuration\n",
    "# ===============================\n",
    "SAVE_PATH = r\"D:\\498R\"   # Adjust path\n",
    "batch_size = 32\n",
    "\n",
    "# ===============================\n",
    "# üíæ Save arrays to disk (once)\n",
    "# ===============================\n",
    "# Example: suppose X_train, y_train, X_val, y_val, X_test, y_test exist in RAM\n",
    "\n",
    "# np.savez_compressed saves multiple arrays under chosen keys\n",
    "np.savez_compressed(os.path.join(SAVE_PATH, \"train(1000)(Cleaned).npz\"), X=X_train, y=y_train)\n",
    "np.savez_compressed(os.path.join(SAVE_PATH, \"val(1000)(Cleaned).npz\"), X=X_val, y=y_val)\n",
    "np.savez_compressed(os.path.join(SAVE_PATH, \"test(1000)(Cleaned).npz\"), X=X_test, y=y_test)\n",
    "\n",
    "print(\"‚úÖ Saved NPZ files.\")\n",
    "\n",
    "# # ===============================\n",
    "# # üìÇ Load datasets from disk\n",
    "# # ===============================\n",
    "# def load_dataset(filename):\n",
    "#     path = os.path.join(SAVE_PATH, filename)\n",
    "#     if not os.path.exists(path):\n",
    "#         raise FileNotFoundError(f\"File not found: {path}\")\n",
    "#     data = np.load(path)\n",
    "#     X, y = data['X'], data['y']\n",
    "#     # Ensure correct dtype for TensorFlow\n",
    "#     X = X.astype('float32')\n",
    "#     y = y.astype('int64')\n",
    "#     return X, y\n",
    "\n",
    "# X_train, y_train = load_dataset(\"train_hybrid_2000.npz\")\n",
    "# X_val, y_val     = load_dataset(\"val_hybrid.npz\")\n",
    "# X_test, y_test   = load_dataset(\"test_hybrid.npz\")\n",
    "\n",
    "# # ===============================\n",
    "# # üîÅ Create tf.data pipelines\n",
    "# # ===============================\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)) \\\n",
    "#     .shuffle(buffer_size=len(X_train), seed=42) \\\n",
    "#     .batch(batch_size) \\\n",
    "#     .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)) \\\n",
    "#     .batch(batch_size) \\\n",
    "#     .cache() \\\n",
    "#     .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)) \\\n",
    "#     .batch(batch_size) \\\n",
    "#     .cache() \\\n",
    "#     .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# # ===============================\n",
    "# # üõ† Sanity checks\n",
    "# # ===============================\n",
    "# print(f\"Train: {X_train.shape} {y_train.shape}\")\n",
    "# print(f\"Val:   {X_val.shape} {y_val.shape}\")\n",
    "# print(f\"Test:  {X_test.shape} {y_test.shape}\")\n",
    "\n",
    "# print(\"Train classes:\", np.unique(y_train))\n",
    "# print(\"Val classes:  \", np.unique(y_val))\n",
    "# print(\"Test classes: \", np.unique(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape, num_labels):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Increased number of filters in Conv1D layers and more layers\n",
    "    x = tf.keras.layers.Conv1D(128, 5, padding='same', activation='relu')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=SEED)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(256, 5, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=SEED)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(512, 3, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=SEED)(x)\n",
    "\n",
    "    # Adding an extra Conv1D layer for more complexity\n",
    "    x = tf.keras.layers.Conv1D(1024, 3, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=SEED)(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.4, seed=SEED)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(num_labels)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_cnn_lstm_model(input_shape, num_labels):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Increased number of filters in Conv1D layers and more layers\n",
    "    x = tf.keras.layers.Conv1D(128, 5, padding='same', activation='relu')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=SEED)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(256, 5, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=SEED)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(512, 3, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=SEED)(x)\n",
    "\n",
    "    # Bidirectional LSTM with more units\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.4, seed=SEED)(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.4, seed=SEED)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(num_labels)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_cnn_gru_model(input_shape, num_labels):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Increased number of filters in Conv1D layers and more layers\n",
    "    x = tf.keras.layers.Conv1D(128, 5, padding='same', activation='relu')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=SEED)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(256, 5, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=SEED)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(512, 3, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=SEED)(x)\n",
    "\n",
    "    # Bidirectional GRU with more units\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(256, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.4, seed=SEED)(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.4, seed=SEED)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(num_labels)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Pure LSTM Model\n",
    "# ======================\n",
    "def create_lstm_model(input_shape, num_labels):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # First LSTM layer\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(inputs)\n",
    "    x = tf.keras.layers.Dropout(0.4, seed=SEED)(x)\n",
    "\n",
    "    # Second LSTM layer\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.4, seed=SEED)(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.4, seed=SEED)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(num_labels)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Pure GRU Model\n",
    "# ======================\n",
    "def create_gru_model(input_shape, num_labels):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # First GRU layer\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(256, return_sequences=True))(inputs)\n",
    "    x = tf.keras.layers.Dropout(0.4, seed=SEED)(x)\n",
    "\n",
    "    # Second GRU layer\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.4, seed=SEED)(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.4, seed=SEED)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(num_labels)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# ======================\n",
    "# GRU + LSTM Hybrid Model\n",
    "# ======================\n",
    "def create_gru_lstm_model(input_shape, num_labels):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # First GRU block\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(256, return_sequences=True))(inputs)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=SEED)(x)\n",
    "\n",
    "    # Then LSTM block\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=SEED)(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.4, seed=SEED)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(num_labels)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input shape (adjust to your dataset)\n",
    "# For speech features: (timesteps, feature_dim), e.g. (126, 155)\n",
    "input_shape = (126, 155)\n",
    "num_labels = 5  # number of emotion classes\n",
    "\n",
    "# ==== CNN ====\n",
    "print(\"\\n===== CNN Model =====\")\n",
    "cnn_model = create_cnn_model(input_shape, num_labels)\n",
    "cnn_model.summary()\n",
    "\n",
    "# ==== CNN + LSTM ====\n",
    "print(\"\\n===== CNN + LSTM Model =====\")\n",
    "cnn_lstm_model = create_cnn_lstm_model(input_shape, num_labels)\n",
    "cnn_lstm_model.summary()\n",
    "\n",
    "# ==== CNN + GRU ====\n",
    "print(\"\\n===== CNN + GRU Model =====\")\n",
    "cnn_gru_model = create_cnn_gru_model(input_shape, num_labels)\n",
    "cnn_gru_model.summary()\n",
    "\n",
    "# ==== Pure LSTM ====\n",
    "print(\"\\n===== LSTM Model =====\")\n",
    "lstm_model = create_lstm_model(input_shape, num_labels)\n",
    "lstm_model.summary()\n",
    "\n",
    "# ==== Pure GRU ====\n",
    "print(\"\\n===== GRU Model =====\")\n",
    "gru_model = create_gru_model(input_shape, num_labels)\n",
    "gru_model.summary()\n",
    "\n",
    "# ==== GRU + LSTM Hybrid ====\n",
    "print(\"\\n===== GRU + LSTM Model =====\")\n",
    "gru_lstm_model = create_gru_lstm_model(input_shape, num_labels)\n",
    "gru_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# --- callbacks: monitor val_accuracy instead of val_f1 ---\n",
    "def create_callbacks(model_name, val_ds=None):\n",
    "    # val_ds kept for signature compatibility; not used here\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',   # keep LR logic on val_loss\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f'best_{model_name}_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return [early_stopping, reduce_lr, model_checkpoint]\n",
    "\n",
    "\n",
    "# --- plot curves after training completes ---\n",
    "def plot_training_curves(history, model_name):\n",
    "    hist = history.history\n",
    "\n",
    "    # Accuracy\n",
    "    if 'accuracy' in hist and 'val_accuracy' in hist:\n",
    "        plt.figure(figsize=(7,5))\n",
    "        plt.plot(hist['accuracy'], label='train_accuracy')\n",
    "        plt.plot(hist['val_accuracy'], label='val_accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'{model_name} - Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{model_name}_accuracy_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "    # Loss\n",
    "    if 'loss' in hist and 'val_loss' in hist:\n",
    "        plt.figure(figsize=(7,5))\n",
    "        plt.plot(hist['loss'], label='train_loss')\n",
    "        plt.plot(hist['val_loss'], label='val_loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'{model_name} - Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{model_name}_loss_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# --- training: same as yours, with callbacks + plotting at the end ---\n",
    "def train_model(model, model_name, train_ds, val_ds, epochs=100):\n",
    "    \"\"\"\n",
    "    IMPORTANT: compile your model with metrics=['accuracy'] \n",
    "    so val_accuracy exists for the callbacks and plots.\n",
    "    \"\"\"\n",
    "    callbacks = create_callbacks(model_name, val_ds)\n",
    "    print(f\"\\nTraining {model_name} model...\")\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    # Load the best-by-val_accuracy weights\n",
    "    model.load_weights(f'best_{model_name}_model.h5')\n",
    "\n",
    "    # Plot curves after training completes\n",
    "    plot_training_curves(history, model_name)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "# --- evaluation: unchanged logic, with a tiny robustness tweak ---\n",
    "def evaluate_model(model, model_name, test_ds, reverse_mapping):\n",
    "    print(f\"\\nEvaluating {model_name} model...\")\n",
    "\n",
    "    # Be robust to metric list shape\n",
    "    eval_out = model.evaluate(test_ds, return_dict=True, verbose=1)\n",
    "    test_loss = float(eval_out.get('loss', np.nan))\n",
    "    test_acc  = float(eval_out.get('accuracy', np.nan))\n",
    "    print(f\"Test loss: {test_loss:.4f}\")\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    # Predictions for confusion matrix and reports\n",
    "    y_pred, y_true = [], []\n",
    "    for batch_x, batch_y in test_ds:\n",
    "        batch_prob = model.predict(batch_x, verbose=0)\n",
    "        batch_pred = np.argmax(batch_prob, axis=1)\n",
    "        y_pred.extend(batch_pred)\n",
    "        y_true.extend(batch_y.numpy())\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "\n",
    "    # Class-wise accuracies\n",
    "    class_accuracies = {}\n",
    "    for i in range(len(reverse_mapping)):\n",
    "        class_indices = np.where(y_true == i)[0]\n",
    "        if len(class_indices) > 0:\n",
    "            class_correct = np.sum(y_pred[class_indices] == i)\n",
    "            class_accuracies[reverse_mapping[i]] = class_correct / len(class_indices)\n",
    "\n",
    "    # (Macro) average over classes (your original logic)\n",
    "    weighted_avg_accuracy = sum(class_accuracies.values()) / len(class_accuracies)\n",
    "\n",
    "    print(f\"\\nWeighted Average Accuracy (WAA): {weighted_avg_accuracy:.4f}\")\n",
    "    print(\"\\nClass-wise accuracies:\")\n",
    "    for emotion, acc in class_accuracies.items():\n",
    "        print(f\"{emotion}: {acc:.4f}\")\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    target_names = [reverse_mapping[i] for i in range(len(reverse_mapping))]\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names,digits = 4))\n",
    "\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"Macro F1-score: {macro_f1:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name}_confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "    return test_acc, weighted_avg_accuracy, macro_f1, class_accuracies, y_pred, y_true, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Train all models\n",
    "# ==============================\n",
    "\n",
    "# CNN\n",
    "cnn_model = create_cnn_model(input_shape, num_labels)\n",
    "cnn_model, cnn_history = train_model(cnn_model, 'cnn', train_ds, val_ds)\n",
    "\n",
    "# CNN-LSTM\n",
    "cnn_lstm_model = create_cnn_lstm_model(input_shape, num_labels)\n",
    "cnn_lstm_model, cnn_lstm_history = train_model(cnn_lstm_model, 'cnn_lstm', train_ds, val_ds)\n",
    "\n",
    "# CNN-GRU\n",
    "cnn_gru_model = create_cnn_gru_model(input_shape, num_labels)\n",
    "cnn_gru_model, cnn_gru_history = train_model(cnn_gru_model, 'cnn_gru', train_ds, val_ds)\n",
    "\n",
    "# Pure LSTM\n",
    "lstm_model = create_lstm_model(input_shape, num_labels)\n",
    "lstm_model, lstm_history = train_model(lstm_model, 'lstm', train_ds, val_ds)\n",
    "\n",
    "# Pure GRU\n",
    "gru_model = create_gru_model(input_shape, num_labels)\n",
    "gru_model, gru_history = train_model(gru_model, 'gru', train_ds, val_ds)\n",
    "\n",
    "# GRU + LSTM Hybrid\n",
    "gru_lstm_model = create_gru_lstm_model(input_shape, num_labels)\n",
    "gru_lstm_model, gru_lstm_history = train_model(gru_lstm_model, 'gru_lstm', train_ds, val_ds)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Evaluate all models\n",
    "# ==============================\n",
    "\n",
    "cnn_results       = evaluate_model(cnn_model, 'cnn', test_ds, reverse_mapping)\n",
    "cnn_lstm_results  = evaluate_model(cnn_lstm_model, 'cnn_lstm', test_ds, reverse_mapping)\n",
    "cnn_gru_results   = evaluate_model(cnn_gru_model, 'cnn_gru', test_ds, reverse_mapping)\n",
    "lstm_results      = evaluate_model(lstm_model, 'lstm', test_ds, reverse_mapping)\n",
    "gru_results       = evaluate_model(gru_model, 'gru', test_ds, reverse_mapping)\n",
    "gru_lstm_results  = evaluate_model(gru_lstm_model, 'gru_lstm', test_ds, reverse_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_training_curves(histories, model_names):\n",
    "    \"\"\"Plot train vs val accuracy/loss for multiple models on the same figure.\"\"\"\n",
    "    plt.figure(figsize=(14,6))\n",
    "    \n",
    "    # --- Accuracy ---\n",
    "    plt.subplot(1,2,1)\n",
    "    for hist, name in zip(histories, model_names):\n",
    "        # handle both History objects and dicts\n",
    "        h = hist.history if hasattr(hist, \"history\") else hist\n",
    "        plt.plot(h['accuracy'], label=f'{name} Train')\n",
    "        plt.plot(h['val_accuracy'], linestyle='--', label=f'{name} Val')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Train vs Validation Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # --- Loss ---\n",
    "    plt.subplot(1,2,2)\n",
    "    for hist, name in zip(histories, model_names):\n",
    "        h = hist.history if hasattr(hist, \"history\") else hist\n",
    "        plt.plot(h['loss'], label=f'{name} Train')\n",
    "        plt.plot(h['val_loss'], linestyle='--', label=f'{name} Val')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Train vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"all_models_training_curves.png\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to create ensemble predictions for any combination of models\n",
    "def create_combination_predictions(models, dataset):\n",
    "    \"\"\"Create ensemble predictions by averaging the softmax outputs of any combination of models.\"\"\"\n",
    "    predictions = [tf.nn.softmax(model.predict(dataset)).numpy() for model in models]\n",
    "    ensemble_preds = np.mean(predictions, axis=0)\n",
    "    return ensemble_preds\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Define all models you want to compare\n",
    "# Get all combinations of models for ensemble (single, pairwise, and full triple)\n",
    "model_combinations = [\n",
    "    [cnn_model],                   # Single model (CNN)\n",
    "    [lstm_model],                  # Single model (LSTM)\n",
    "    [gru_model],                   # Single model (GRU)\n",
    "    [cnn_model, lstm_model],       # Pairwise (CNN + LSTM)\n",
    "    [cnn_model, gru_model],        # Pairwise (CNN + GRU)\n",
    "    [lstm_model, gru_model],       # Pairwise (LSTM + GRU)\n",
    "    [cnn_model, lstm_model, gru_model]  # Triple ensemble (CNN + LSTM + GRU)\n",
    "]\n",
    "\n",
    "# Model names for plotting\n",
    "model_names = [\n",
    "    'CNN',\n",
    "    'LSTM',\n",
    "    'GRU',\n",
    "    'CNN + LSTM',\n",
    "    'CNN + GRU',\n",
    "    'LSTM + GRU',\n",
    "    'CNN + LSTM + GRU'   # ‚úÖ Fixed naming for triple ensemble\n",
    "]\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Calculate accuracy and WAA for each combination\n",
    "# ======================================================\n",
    "accuracies = []\n",
    "waas = []\n",
    "\n",
    "true_labels = np.concatenate([y for x, y in test_ds], axis=0)  # ground truth once\n",
    "\n",
    "for models in model_combinations:\n",
    "    ensemble_preds = create_combination_predictions(models, test_ds)\n",
    "    ensemble_pred_classes = np.argmax(ensemble_preds, axis=1)\n",
    "\n",
    "    # Accuracy\n",
    "    ensemble_acc = np.mean(ensemble_pred_classes == true_labels)\n",
    "    accuracies.append(ensemble_acc)\n",
    "\n",
    "    # Weighted Average Accuracy (macro over classes)\n",
    "    ensemble_class_accuracies = {}\n",
    "    for cls in range(num_labels):\n",
    "        mask = true_labels == cls\n",
    "        if np.sum(mask) > 0:\n",
    "            class_acc = np.mean(ensemble_pred_classes[mask] == true_labels[mask])\n",
    "            ensemble_class_accuracies[reverse_mapping[cls]] = class_acc\n",
    "    ensemble_waa = np.mean(list(ensemble_class_accuracies.values()))\n",
    "    waas.append(ensemble_waa)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Visualization (Horizontal Bar Plots)\n",
    "# ======================================================\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# --- Accuracy ---\n",
    "plt.subplot(1, 2, 1)\n",
    "bars1 = plt.barh(model_names, accuracies, color=plt.cm.Set2.colors, alpha=0.9)\n",
    "plt.xlabel('Accuracy', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Model', fontsize=16, fontweight='bold')\n",
    "plt.title('Test Accuracy Comparison', fontsize=18, fontweight='bold')\n",
    "for i, bar in enumerate(bars1):\n",
    "    plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,\n",
    "             f'{accuracies[i]:.4f}', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# --- WAA ---\n",
    "plt.subplot(1, 2, 2)\n",
    "bars2 = plt.barh(model_names, waas, color=plt.cm.Set2.colors, alpha=0.9)\n",
    "plt.xlabel('Weighted Average Accuracy', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Model', fontsize=16, fontweight='bold')\n",
    "plt.title('Weighted Average Accuracy Comparison', fontsize=18, fontweight='bold')\n",
    "for i, bar in enumerate(bars2):\n",
    "    plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,\n",
    "             f'{waas[i]:.4f}', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Save results to file\n",
    "# ======================================================\n",
    "with open('model_results.txt', 'w') as f:\n",
    "    f.write(\"Speech Emotion Recognition Model Results\\n\")\n",
    "    f.write(\"=======================================\\n\\n\")\n",
    "    f.write(\"Summary of Results:\\n\")\n",
    "    f.write(\"-----------------\\n\")\n",
    "    f.write(f\"Best model by accuracy: {model_names[np.argmax(accuracies)]} ({max(accuracies):.4f})\\n\")\n",
    "    f.write(f\"Best model by WAA: {model_names[np.argmax(waas)]} ({max(waas):.4f})\\n\\n\")\n",
    "    f.write(\"Detailed Model Performance:\\n\")\n",
    "    f.write(\"-------------------------\\n\")\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        f.write(f\"{model_name} Model:\\n\")\n",
    "        f.write(f\"  Test Accuracy: {accuracies[i]:.4f}\\n\")\n",
    "        f.write(f\"  Weighted Avg Accuracy (WAA): {waas[i]:.4f}\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =====================\n",
    "# Example Data (replace with your real results)\n",
    "# =====================\n",
    "model_names = ['CNN', 'CNN+LSTM', 'CNN+GRU', 'LSTM', 'GRU', 'LSTM+GRU']\n",
    "accuracies = [0.9680, 0.9620, 0.9680, 0.9380, 0.9500, 0.9600]\n",
    "# waas = [0.9680, 0.9620, 0.9680, 0.9380, 0.9500, 0.9600]\n",
    "\n",
    "# =====================\n",
    "# Plot 1: Test Accuracy\n",
    "# =====================\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars1 = plt.barh(model_names, accuracies, color=plt.cm.Set2.colors, alpha=0.9)\n",
    "\n",
    "# Labels and Title\n",
    "plt.xlabel('Accuracy', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Model', fontsize=16, fontweight='bold')\n",
    "plt.title('Test Accuracy Comparison', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Show values on bars\n",
    "for i, bar in enumerate(bars1):\n",
    "    plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,\n",
    "             f'{accuracies[i]:.4f}', va='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.box(False)  # Remove plot box\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"test_accuracy_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # =====================\n",
    "# # Plot 2: Weighted Average Accuracy\n",
    "# # =====================\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# bars2 = plt.barh(model_names, waas, color=plt.cm.Set2.colors, alpha=0.9)\n",
    "\n",
    "# # Labels and Title\n",
    "# plt.xlabel('Weighted Average Accuracy', fontsize=16, fontweight='bold')\n",
    "# plt.ylabel('Model', fontsize=16, fontweight='bold')\n",
    "# plt.title('Weighted Average Accuracy Comparison', fontsize=18, fontweight='bold')\n",
    "\n",
    "# # Show values on bars\n",
    "# for i, bar in enumerate(bars2):\n",
    "#     plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,\n",
    "#              f'{waas[i]:.4f}', va='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "# plt.box(False)  # Remove plot box\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"waa_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define input shape and number of labels ===\n",
    "input_shape = (126, 155)\n",
    "num_labels = 5\n",
    "\n",
    "# === Load CNN ===\n",
    "cnn_model = create_cnn_model(input_shape, num_labels)\n",
    "cnn_model.load_weights('best_cnn_model.h5')\n",
    "\n",
    "# === Load LSTM ===\n",
    "lstm_model = create_lstm_model(input_shape, num_labels)\n",
    "lstm_model.load_weights('best_lstm_model.h5')\n",
    "\n",
    "# === Load GRU ===\n",
    "gru_model = create_gru_model(input_shape, num_labels)\n",
    "gru_model.load_weights('best_gru_model.h5')\n",
    "\n",
    "# === Load CNN + LSTM ===\n",
    "cnn_lstm_model = create_cnn_lstm_model(input_shape, num_labels)\n",
    "cnn_lstm_model.load_weights('best_cnn_lstm_model.h5')\n",
    "\n",
    "# === Load CNN + GRU ===\n",
    "cnn_gru_model = create_cnn_gru_model(input_shape, num_labels)\n",
    "cnn_gru_model.load_weights('best_cnn_gru_model.h5')\n",
    "\n",
    "# === Load GRU + LSTM ===\n",
    "gru_lstm_model = create_gru_lstm_model(input_shape, num_labels)\n",
    "gru_lstm_model.load_weights('best_gru_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def collect_preds_and_labels(model, test_ds, num_labels):\n",
    "    \"\"\"Return y_true(int), y_prob(probabilities [N, C]).\"\"\"\n",
    "    y_true_list, y_prob_list = [], []\n",
    "    for x, y in test_ds:\n",
    "        # Convert one-hot labels to int if necessary\n",
    "        if len(y.shape) > 1 and y.shape[-1] == num_labels:\n",
    "            y_true_batch = np.argmax(y.numpy(), axis=1)\n",
    "        else:\n",
    "            y_true_batch = y.numpy().astype(int).reshape(-1)\n",
    "\n",
    "        prob = model.predict(x, verbose=0)\n",
    "        y_true_list.append(y_true_batch)\n",
    "        y_prob_list.append(prob)\n",
    "\n",
    "    y_true = np.concatenate(y_true_list, axis=0)\n",
    "    y_prob = np.concatenate(y_prob_list, axis=0)\n",
    "\n",
    "    # Ensure probabilities\n",
    "    if not np.allclose(np.sum(y_prob, axis=1), 1.0, atol=1e-3):\n",
    "        y_prob = tf.nn.softmax(y_prob, axis=1).numpy()\n",
    "\n",
    "    return y_true, y_prob\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, model_name, normalize=False):\n",
    "    \"\"\"Green variant confusion matrix with bold axis labels.\"\"\"\n",
    "    if normalize:\n",
    "        cm_disp = cm.astype('float') / cm.sum(axis=1, keepdims=True).clip(min=1e-12)\n",
    "        fmt = \".2f\"\n",
    "        title = f\"Confusion Matrix (Normalized) - {model_name}\"\n",
    "    else:\n",
    "        cm_disp = cm\n",
    "        fmt = \"d\"\n",
    "        title = f\"Confusion Matrix - {model_name}\"\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    im = plt.imshow(cm_disp, interpolation='nearest', cmap='Greens')\n",
    "    plt.title(title, fontweight='bold')\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45, ha='right', fontweight='bold')\n",
    "    plt.yticks(tick_marks, class_names, fontweight='bold')\n",
    "\n",
    "    thresh = cm_disp.max() / 2.0\n",
    "    for i in range(cm_disp.shape[0]):\n",
    "        for j in range(cm_disp.shape[1]):\n",
    "            plt.text(j, i, format(cm_disp[i, j], fmt),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm_disp[i, j] > thresh else \"black\",\n",
    "                     fontweight='bold')\n",
    "\n",
    "    plt.ylabel('True label', fontweight='bold')\n",
    "    plt.xlabel('Predicted label', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curves(y_true, y_prob, class_names, model_name):\n",
    "    \"\"\"One-vs-rest ROC for each class + micro and macro averages.\"\"\"\n",
    "    num_labels = len(class_names)\n",
    "\n",
    "    y_true_bin = label_binarize(y_true, classes=list(range(num_labels)))\n",
    "    if y_true_bin.shape[1] < num_labels:\n",
    "        pad = np.zeros((y_true_bin.shape[0], num_labels - y_true_bin.shape[1]))\n",
    "        y_true_bin = np.concatenate([y_true_bin, pad], axis=1)\n",
    "\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "\n",
    "    for i in range(num_labels):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Micro-average\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_prob.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Macro-average\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_labels)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(num_labels):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= num_labels\n",
    "    fpr[\"macro\"], tpr[\"macro\"], roc_auc[\"macro\"] = all_fpr, mean_tpr, auc(all_fpr, mean_tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', linewidth=1, color='gray')\n",
    "\n",
    "    for i in range(num_labels):\n",
    "        plt.plot(fpr[i], tpr[i], label=f'{class_names[i]} (AUC = {roc_auc[i]:.3f})')\n",
    "\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], linewidth=2, label=f'micro-average (AUC = {roc_auc[\"micro\"]:.3f})')\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"], linewidth=2, label=f'macro-average (AUC = {roc_auc[\"macro\"]:.3f})')\n",
    "\n",
    "    plt.xlabel('False Positive Rate', fontweight='bold')\n",
    "    plt.ylabel('True Positive Rate', fontweight='bold')\n",
    "    plt.title(f'ROC Curves - {model_name}', fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize='small')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nAUC per class:\")\n",
    "    for i in range(num_labels):\n",
    "        print(f\"  {class_names[i]}: {roc_auc[i]:.4f}\")\n",
    "    print(f\"Micro-average AUC: {roc_auc['micro']:.4f}\")\n",
    "    print(f\"Macro-average AUC: {roc_auc['macro']:.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_full(model, model_name, test_ds, class_names):\n",
    "    num_labels = len(class_names)\n",
    "    y_true, y_prob = collect_preds_and_labels(model, test_ds, num_labels)\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "    print(f\"\\n=== {model_name} : Classification Report ===\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_labels)))\n",
    "    plot_confusion_matrix(cm, class_names, model_name, normalize=False)\n",
    "    plot_confusion_matrix(cm, class_names, model_name, normalize=True)\n",
    "\n",
    "    plot_roc_curves(y_true, y_prob, class_names, model_name)\n",
    "\n",
    "\n",
    "# ---------- usage ----------\n",
    "class_names = [\"happy\", \"sad\", \"angry\", \"surprise\", \"neutral\"]\n",
    "\n",
    "# Evaluate all six models\n",
    "evaluate_full(cnn_model,       \"CNN\",        test_ds, class_names)\n",
    "evaluate_full(lstm_model,      \"LSTM\",       test_ds, class_names)\n",
    "evaluate_full(gru_model,       \"GRU\",        test_ds, class_names)\n",
    "evaluate_full(cnn_lstm_model,  \"CNN+LSTM\",   test_ds, class_names)\n",
    "evaluate_full(cnn_gru_model,   \"CNN+GRU\",    test_ds, class_names)\n",
    "evaluate_full(gru_lstm_model,  \"GRU+LSTM\",   test_ds, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "# ---------- Ensemble helpers ----------\n",
    "def get_model_preds(models, test_ds, num_labels):\n",
    "    \"\"\"Return list of (y_true, y_prob) for each model on test_ds.\"\"\"\n",
    "    all_preds = []\n",
    "    for m in models:\n",
    "        y_true, y_prob = collect_preds_and_labels(m, test_ds, num_labels)\n",
    "        all_preds.append((y_true, y_prob))\n",
    "    return all_preds\n",
    "\n",
    "\n",
    "def ensemble_predictions(all_preds, method=\"mean\"):\n",
    "    \"\"\"\n",
    "    Combine predictions from multiple models.\n",
    "    all_preds: list of (y_true, y_prob)\n",
    "    method: 'mean' | 'maxvote' | 'confidence'\n",
    "    \"\"\"\n",
    "    # All y_true should be the same\n",
    "    y_true = all_preds[0][0]\n",
    "    probs = [p for (_, p) in all_preds]\n",
    "    num_models = len(probs)\n",
    "\n",
    "    if method == \"mean\":\n",
    "        y_prob = np.mean(probs, axis=0)\n",
    "\n",
    "    elif method == \"maxvote\":\n",
    "        preds = [np.argmax(p, axis=1) for p in probs]\n",
    "        y_pred = []\n",
    "        for i in range(len(y_true)):\n",
    "            votes = [pred[i] for pred in preds]\n",
    "            most_common = Counter(votes).most_common(1)[0][0]\n",
    "            y_pred.append(most_common)\n",
    "        y_pred = np.array(y_pred)\n",
    "        # make one-hot like probabilities\n",
    "        y_prob = np.zeros_like(probs[0])\n",
    "        y_prob[np.arange(len(y_pred)), y_pred] = 1.0\n",
    "\n",
    "    elif method == \"confidence\":\n",
    "        y_prob = np.zeros_like(probs[0])\n",
    "        y_pred = []\n",
    "        for i in range(len(y_true)):\n",
    "            # Pick model with highest confidence\n",
    "            best_idx = np.argmax([np.max(p[i]) for p in probs])\n",
    "            y_prob[i] = probs[best_idx][i]\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown method\")\n",
    "\n",
    "    return y_true, y_prob\n",
    "\n",
    "\n",
    "def evaluate_ensemble(all_preds, method, class_names, combo_name):\n",
    "    y_true, y_prob = ensemble_predictions(all_preds, method=method)\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "    print(f\"\\n=== Ensemble ({method}) on {combo_name} ===\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
    "    plot_confusion_matrix(cm, class_names, f\"Ensemble-{method}-{combo_name}\", normalize=False)\n",
    "    plot_confusion_matrix(cm, class_names, f\"Ensemble-{method}-{combo_name}\", normalize=True)\n",
    "\n",
    "    plot_roc_curves(y_true, y_prob, class_names, f\"Ensemble-{method}-{combo_name}\")\n",
    "\n",
    "\n",
    "# ---------- Usage ----------\n",
    "class_names = [\"happy\", \"sad\", \"angry\", \"surprise\", \"neutral\"]\n",
    "\n",
    "# list of models (already loaded with weights)\n",
    "models = [\n",
    "    cnn_model,\n",
    "    lstm_model,\n",
    "    gru_model,\n",
    "    cnn_lstm_model,\n",
    "    cnn_gru_model,\n",
    "    gru_lstm_model\n",
    "]\n",
    "\n",
    "# all model predictions\n",
    "num_labels = len(class_names)\n",
    "all_preds = get_model_preds(models, test_ds, num_labels)\n",
    "\n",
    "# Try ensembles on ALL models together\n",
    "for method in [\"mean\", \"maxvote\", \"confidence\"]:\n",
    "    evaluate_ensemble(all_preds, method, class_names, combo_name=\"All_6\")\n",
    "\n",
    "# If you want all possible COMBINATIONS (pairs, triplets, etc.)\n",
    "for r in range(2, len(models) + 1):\n",
    "    for subset_idx in itertools.combinations(range(len(models)), r):\n",
    "        subset_models = [all_preds[i] for i in subset_idx]\n",
    "        combo_name = \"+\".join([f\"M{i+1}\" for i in subset_idx])\n",
    "        for method in [\"mean\", \"maxvote\", \"confidence\"]:\n",
    "            evaluate_ensemble(subset_models, method, class_names, combo_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# ---------- Ensemble helpers ----------\n",
    "def get_model_preds(models, test_ds, num_labels):\n",
    "    \"\"\"Return list of (y_true, y_prob) for each model on test_ds.\"\"\"\n",
    "    all_preds = []\n",
    "    for m in models:\n",
    "        y_true, y_prob = collect_preds_and_labels(m, test_ds, num_labels)\n",
    "        all_preds.append((y_true, y_prob))\n",
    "    return all_preds\n",
    "\n",
    "\n",
    "def ensemble_predictions(all_preds, method=\"mean\"):\n",
    "    \"\"\"Combine predictions from multiple models.\"\"\"\n",
    "    y_true = all_preds[0][0]\n",
    "    probs = [p for (_, p) in all_preds]\n",
    "\n",
    "    if method == \"mean\":\n",
    "        y_prob = np.mean(probs, axis=0)\n",
    "\n",
    "    elif method == \"maxvote\":\n",
    "        preds = [np.argmax(p, axis=1) for p in probs]\n",
    "        y_pred = []\n",
    "        for i in range(len(y_true)):\n",
    "            votes = [pred[i] for pred in preds]\n",
    "            most_common = Counter(votes).most_common(1)[0][0]\n",
    "            y_pred.append(most_common)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_prob = np.zeros_like(probs[0])\n",
    "        y_prob[np.arange(len(y_pred)), y_pred] = 1.0\n",
    "\n",
    "    elif method == \"confidence\":\n",
    "        y_prob = np.zeros_like(probs[0])\n",
    "        for i in range(len(y_true)):\n",
    "            best_idx = np.argmax([np.max(p[i]) for p in probs])\n",
    "            y_prob[i] = probs[best_idx][i]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown method\")\n",
    "\n",
    "    return y_true, y_prob\n",
    "\n",
    "\n",
    "def evaluate_ensemble(all_preds, method, class_names, combo_name):\n",
    "    \"\"\"Evaluate ensemble and print metrics + plots.\"\"\"\n",
    "    y_true, y_prob = ensemble_predictions(all_preds, method=method)\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    rec  = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    prec = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    f1   = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    print(f\"\\n=== Ensemble ({method}) on {combo_name} ===\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"F1 score:  {f1:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
    "    plot_confusion_matrix(cm, class_names, f\"Ensemble-{method}-{combo_name}\", normalize=False)\n",
    "    plot_confusion_matrix(cm, class_names, f\"Ensemble-{method}-{combo_name}\", normalize=True)\n",
    "\n",
    "    plot_roc_curves(y_true, y_prob, class_names, f\"Ensemble-{method}-{combo_name}\")\n",
    "\n",
    "\n",
    "# ---------- Usage ----------\n",
    "class_names = [\"happy\", \"sad\", \"angry\", \"surprise\", \"neutral\"]\n",
    "\n",
    "# Only using CNN, CNN-GRU, CNN-LSTM\n",
    "models = [cnn_model, cnn_gru_model, cnn_lstm_model]\n",
    "num_labels = len(class_names)\n",
    "\n",
    "# Collect predictions once\n",
    "all_preds = get_model_preds(models, test_ds, num_labels)\n",
    "\n",
    "# Model labels for pretty printing\n",
    "model_names = [\"CNN\", \"CNN_GRU\", \"CNN_LSTM\"]\n",
    "\n",
    "# Iterate over all combinations (pairs and all 3)\n",
    "for r in range(2, len(models) + 1):\n",
    "    for subset_idx in itertools.combinations(range(len(models)), r):\n",
    "        subset_preds = [all_preds[i] for i in subset_idx]\n",
    "        combo_name = \"+\".join([model_names[i] for i in subset_idx])\n",
    "        for method in [\"mean\", \"maxvote\", \"confidence\"]:\n",
    "            evaluate_ensemble(subset_preds, method, class_names, combo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# ---------- Helper to collect preds ----------\n",
    "def collect_model_preds(model, test_ds, num_labels):\n",
    "    y_true, y_prob = collect_preds_and_labels(model, test_ds, num_labels)\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "    return y_true, y_pred\n",
    "\n",
    "def save_conf_matrix_norm(y_true, y_pred, class_names, title, save_dir=\"conf_matrices_norm\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f\"Normalized Confusion Matrix - {title}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    \n",
    "    # ‚úÖ Save without cutting off labels\n",
    "    plt.savefig(os.path.join(save_dir, f\"{title}_cm_norm.png\"), bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# ---------- Class names ----------\n",
    "class_names = [\"happy\", \"sad\", \"angry\", \"surprise\", \"neutral\"]\n",
    "num_labels = len(class_names)\n",
    "\n",
    "# ---------- Your 6 models (already loaded with .h5 weights) ----------\n",
    "models = {\n",
    "    \"CNN\": cnn_model,\n",
    "    \"CNN_LSTM\": cnn_lstm_model,\n",
    "    \"CNN_GRU\": cnn_gru_model,\n",
    "    \"GRU\": gru_model,\n",
    "    \"LSTM\": lstm_model,\n",
    "    \"GRU_LSTM\": gru_lstm_model\n",
    "}\n",
    "\n",
    "# ---------- Evaluate & Save ----------\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== Evaluating {name} ===\")\n",
    "    y_true, y_pred = collect_model_preds(model, test_ds, num_labels)\n",
    "\n",
    "    # Print report to console\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "    # Save only normalized confusion matrix\n",
    "    save_conf_matrix_norm(y_true, y_pred, class_names, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7353176,
     "sourceId": 11714488,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
