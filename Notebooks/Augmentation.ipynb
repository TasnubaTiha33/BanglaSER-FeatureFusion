{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# üîÅ Full Reproducibility + Warning Suppression Setup\n",
    "# ==================================================\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# === ENVIRONMENT VARIABLES (SET BEFORE TF IMPORT) ===\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)           # Hash seed\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'           # Suppress TensorFlow logs\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '0'           # Allow non-deterministic ops to avoid UnimplementedError\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'           # Set GPU ID (or \"\" to force CPU)\n",
    "\n",
    "# === PYTHON SEED SETTINGS ===\n",
    "random.seed(SEED)\n",
    "\n",
    "# === SUPPRESS WARNINGS & LOGGING ===\n",
    "logging.getLogger('absl').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# === IMPORT LIBRARIES AFTER SEED SETTINGS ===\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === NUMPY & TENSORFLOW SEEDS ===\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# === SINGLE-THREADING FOR FULL REPRODUCIBILITY ===\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "# ‚úÖ CHECK GPU AVAILABILITY\n",
    "print(\"‚úÖ GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# ‚úÖ DATASET PATH\n",
    "DATASET_PATH = r'D:\\498R\\BanglaSER'\n",
    "\n",
    "# ‚úÖ EMOTION LABELS FROM FILENAME\n",
    "# Format: Mode-StatementType-Emotion-Intensity-Statement-Repetition-Actor.wav\n",
    "EMOTION_MAPPING = {\n",
    "    '01': 'happy',\n",
    "    '02': 'sad',\n",
    "    '03': 'angry',\n",
    "    '04': 'surprise',\n",
    "    '05': 'neutral'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset information from file structure\n",
    "def create_dataset_info(root_path):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                # Parse the filename to get emotion\n",
    "                # Format: Mode-StatementType-Emotion-Intensity-Statement-Repetition-Actor.wav\n",
    "                parts = file.split('-')\n",
    "                if len(parts) == 7:\n",
    "                    emotion_code = parts[2]\n",
    "                    emotion = EMOTION_MAPPING.get(emotion_code, 'unknown')\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    data.append({\n",
    "                        'file_path': file_path,\n",
    "                        'emotion': emotion,\n",
    "                        'emotion_code': emotion_code\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create and display dataset info\n",
    "df = create_dataset_info(DATASET_PATH)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(df['emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get class distribution\n",
    "emotion_counts = df['emotion'].value_counts()\n",
    "\n",
    "# Pie chart\n",
    "fig, ax = plt.subplots(figsize=(8,8), facecolor='none')  # transparent figure background\n",
    "ax.set_facecolor('none')  # transparent axes background\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    emotion_counts, \n",
    "    labels=emotion_counts.index, \n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90, \n",
    "    colors=plt.cm.Set3.colors,\n",
    "    textprops={'fontsize': 19, 'fontweight': 'bold'}\n",
    ")\n",
    "\n",
    "# Bolden the percentages\n",
    "for autotext in autotexts:\n",
    "    autotext.set_fontsize(19)\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "# ---------------------------\n",
    "# Audio loading & utilities\n",
    "# ---------------------------\n",
    "TARGET_SR = 16000\n",
    "DURATION_SEC = 4.0\n",
    "TARGET_LEN = int(TARGET_SR * DURATION_SEC)\n",
    "\n",
    "def load_audio_fixed(file_path, sr=TARGET_SR, duration=DURATION_SEC):\n",
    "    audio, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
    "    if len(audio) < TARGET_LEN:\n",
    "        audio = np.pad(audio, (0, TARGET_LEN - len(audio)), mode='constant')\n",
    "    else:\n",
    "        audio = audio[:TARGET_LEN]\n",
    "    return audio, sr\n",
    "\n",
    "def ensure_length(audio, target_len=TARGET_LEN):\n",
    "    if len(audio) < target_len:\n",
    "        audio = np.pad(audio, (0, target_len - len(audio)), mode='constant')\n",
    "    elif len(audio) > target_len:\n",
    "        audio = audio[:target_len]\n",
    "    return audio\n",
    "\n",
    "# ---------------------------\n",
    "# Audio-domain augmentations\n",
    "# ---------------------------\n",
    "def aug_noise(audio, noise_std=0.005):\n",
    "    return audio + np.random.normal(0.0, noise_std, size=audio.shape)\n",
    "\n",
    "def aug_shift(audio, max_shift_ratio=0.1):\n",
    "    \"\"\"Zero-pad shift (no wrap-around).\"\"\"\n",
    "    max_shift = int(len(audio) * max_shift_ratio)\n",
    "    shift = np.random.randint(-max_shift, max_shift + 1)\n",
    "    if shift == 0:\n",
    "        return audio\n",
    "    if shift > 0:\n",
    "        # delay: pad at start, drop end\n",
    "        return np.concatenate([np.zeros(shift, dtype=audio.dtype), audio[:-shift]])\n",
    "    else:\n",
    "        # advance: drop start, pad at end\n",
    "        s = -shift\n",
    "        return np.concatenate([audio[s:], np.zeros(s, dtype=audio.dtype)])\n",
    "\n",
    "def aug_pitch(audio, sr=TARGET_SR, semitone_range=(-2, 2)):\n",
    "    steps = np.random.uniform(semitone_range[0], semitone_range[1])\n",
    "    y = librosa.effects.pitch_shift(y=audio, sr=sr, n_steps=steps)\n",
    "    return ensure_length(y, TARGET_LEN)\n",
    "\n",
    "def aug_stretch(audio, rate_range=(0.8, 1.25)):\n",
    "    rate = np.random.uniform(rate_range[0], rate_range[1])\n",
    "    # In some librosa versions, 'rate' is keyword-only\n",
    "    y = librosa.effects.time_stretch(y=audio, rate=rate)\n",
    "    return ensure_length(y, TARGET_LEN)\n",
    "\n",
    "AUG_FUNCS = ['noise', 'shift', 'pitch', 'stretch']\n",
    "\n",
    "def apply_aug(audio, method):\n",
    "    if method == 'noise':\n",
    "        return aug_noise(audio)\n",
    "    elif method == 'shift':\n",
    "        return aug_shift(audio)\n",
    "    elif method == 'pitch':\n",
    "        return aug_pitch(audio)\n",
    "    elif method == 'stretch':\n",
    "        return aug_stretch(audio)\n",
    "    return audio\n",
    "\n",
    "# ---------------------------\n",
    "# Feature extraction (unchanged logic, but from audio)\n",
    "# ---------------------------\n",
    "def extract_features_from_audio(audio, sr=TARGET_SR):\n",
    "    mfccs   = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    mel_s   = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)\n",
    "    log_mel = librosa.power_to_db(mel_s)\n",
    "    zcr     = librosa.feature.zero_crossing_rate(audio)\n",
    "    chroma  = librosa.feature.chroma_stft(y=audio, sr=sr, n_chroma=12)\n",
    "    rms     = librosa.feature.rms(y=audio)\n",
    "\n",
    "    features = np.vstack([mfccs, log_mel, zcr, chroma, rms])  # (155, T)\n",
    "    return features.T  # (T, 155)\n",
    "\n",
    "# If you need the same signature as before:\n",
    "def extract_features(file_path):\n",
    "    audio, sr = load_audio_fixed(file_path, sr=TARGET_SR, duration=DURATION_SEC)\n",
    "    return extract_features_from_audio(audio, sr)\n",
    "\n",
    "# ---------------------------\n",
    "# Label mapping (same as yours)\n",
    "# ---------------------------\n",
    "emotions = df['emotion'].unique()\n",
    "label_mapping = {label: i for i, label in enumerate(emotions)}\n",
    "reverse_mapping = {i: label for label, i in label_mapping.items()}\n",
    "num_labels = len(emotions)\n",
    "\n",
    "print(\"\\nEmotion Label Mapping:\")\n",
    "for emotion, idx in label_mapping.items():\n",
    "    print(f\"{emotion}: {idx}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# PREPROCESS + AUDIO AUGMENTATION (before split)\n",
    "# ---------------------------------------------\n",
    "print(\"üîÅ Extracting ORIGINAL features for ALL files ...\")\n",
    "X_all, y_all = [], []\n",
    "audios_by_class = defaultdict(list)\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"All data\"):\n",
    "    audio, sr = load_audio_fixed(row['file_path'], sr=TARGET_SR, duration=DURATION_SEC)\n",
    "    # keep raw audio per class for later augmentation\n",
    "    label = label_mapping[row['emotion']]\n",
    "    audios_by_class[label].append(audio)\n",
    "\n",
    "    # original features\n",
    "    feats = extract_features_from_audio(audio, sr)\n",
    "    X_all.append(feats)\n",
    "    y_all.append(label)\n",
    "\n",
    "print(f\"üìä Original samples: {len(X_all)}\")\n",
    "\n",
    "# Class-balanced augmentation over WHOLE set (audio-domain)\n",
    "target_per_class = 1000  # your target\n",
    "aug_X_all, aug_y_all = [], []\n",
    "\n",
    "print(\"üéõÔ∏è Augmenting (audio-domain) to balance classes ...\")\n",
    "for label, audios in audios_by_class.items():\n",
    "    count = len(audios)\n",
    "    needed = max(0, target_per_class - count)\n",
    "    i = 0\n",
    "    while needed > 0:\n",
    "        base = audios[i % count]\n",
    "        method = AUG_FUNCS[np.random.randint(0, len(AUG_FUNCS))]\n",
    "        aug_audio = apply_aug(base, method)\n",
    "        aug_audio = ensure_length(aug_audio, TARGET_LEN)  # safety\n",
    "        feats = extract_features_from_audio(aug_audio, TARGET_SR)\n",
    "        aug_X_all.append(feats)\n",
    "        aug_y_all.append(label)\n",
    "        i += 1\n",
    "        needed -= 1\n",
    "\n",
    "# Combine originals + augmented (BEFORE split)\n",
    "X_all = np.concatenate([np.array(X_all), np.array(aug_X_all)], axis=0)\n",
    "y_all = np.concatenate([np.array(y_all), np.array(aug_y_all)], axis=0)\n",
    "\n",
    "print(f\"‚úÖ Final samples after class-balanced augmentation: {X_all.shape[0]} (shape per sample: {X_all.shape[1:]} )\")\n",
    "\n",
    "# -----------------\n",
    "# Split (as you want)\n",
    "# -----------------\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train)} | Val set: {len(X_val)} | Test set: {len(X_test)}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Feature scaling (fit on TRAIN only)\n",
    "# ---------------------------------------\n",
    "# Flatten across time to fit a scaler on feature dims\n",
    "T, F = X_train.shape[1], X_train.shape[2]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.reshape(-1, F))  # train-only fit\n",
    "\n",
    "def transform_with_scaler(X, scaler):\n",
    "    N, T, F = X.shape\n",
    "    X2 = X.reshape(-1, F)\n",
    "    X2 = scaler.transform(X2)\n",
    "    return X2.reshape(N, T, F)\n",
    "\n",
    "X_train = transform_with_scaler(X_train, scaler)\n",
    "X_val   = transform_with_scaler(X_val, scaler)\n",
    "X_test  = transform_with_scaler(X_test, scaler)\n",
    "\n",
    "# -------------------------\n",
    "# Build TensorFlow datasets\n",
    "# -------------------------\n",
    "batch_size = 32\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds   = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds  = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"\\n‚úÖ Dataset preparation complete (AUGMENT ‚Üí SPLIT).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Pick one sample (e.g., first training sample)\n",
    "idx = 0  \n",
    "audio = X_train[idx].reshape(-1)  \n",
    "\n",
    "# Extract individual features\n",
    "mfccs   = librosa.feature.mfcc(y=audio, sr=TARGET_SR, n_mfcc=13)\n",
    "mel_s   = librosa.feature.melspectrogram(y=audio, sr=TARGET_SR, n_mels=128)\n",
    "log_mel = librosa.power_to_db(mel_s)\n",
    "zcr     = librosa.feature.zero_crossing_rate(audio)\n",
    "chroma  = librosa.feature.chroma_stft(y=audio, sr=TARGET_SR, n_chroma=12)\n",
    "rms     = librosa.feature.rms(y=audio)\n",
    "\n",
    "features_dict = {\n",
    "    \"MFCCs\": (mfccs, \"magma\"),\n",
    "    \"Log-Mel\": (log_mel, \"inferno\"),\n",
    "    \"ZCR\": (zcr, \"plasma\"),\n",
    "    \"Chroma\": (chroma, \"cividis\"),\n",
    "    \"RMS\": (rms, \"coolwarm\")\n",
    "}\n",
    "\n",
    "# Folder to save images\n",
    "os.makedirs(\"feature_heatmaps_v2\", exist_ok=True)\n",
    "\n",
    "# Save each feature as a separate heatmap with new color + name\n",
    "for name, (feat, cmap) in features_dict.items():\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(feat, aspect='auto', origin='lower', cmap=cmap)\n",
    "    plt.title(f\"{name}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.colorbar(format=\"%+2.1f dB\" if name==\"Log-Mel\" else None)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        f\"feature_heatmaps_v2/{name}_v2.png\",  # different filenames\n",
    "        transparent=True,\n",
    "        dpi=300\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "print(\"‚úÖ Saved 5 new heatmaps with different colors in 'feature_heatmaps_v2/' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example: your real counts\n",
    "before_counts = [len(audios) for label, audios in audios_by_class.items()]\n",
    "after_counts  = [1000 for _ in audios_by_class]   # since you balanced to 1000\n",
    "class_labels  = [reverse_mapping[label] for label in audios_by_class.keys()]\n",
    "\n",
    "x = np.arange(len(class_labels))\n",
    "width = 0.5  # üîπ Increased thickness\n",
    "\n",
    "# üîπ Transparent figure + axes\n",
    "fig, ax = plt.subplots(figsize=(10,6), facecolor='none')\n",
    "ax.set_facecolor('none')\n",
    "\n",
    "# Plot bars\n",
    "rects1 = ax.bar(x - width/2, before_counts, width, \n",
    "                label=\"Before Augmentation\", color='skyblue')\n",
    "rects2 = ax.bar(x + width/2, after_counts, width, \n",
    "                label=\"After Augmentation\", color='lightcoral')\n",
    "\n",
    "# Titles and labels\n",
    "ax.set_ylabel(\"Number of Samples\", fontsize=16, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_labels, rotation=0, fontsize=14, fontweight='bold')  # üîπ No tilt\n",
    "ax.tick_params(axis='y', labelsize=14, width=2)\n",
    "\n",
    "# Add value labels above bars\n",
    "def autolabel(rects, color):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height}',\n",
    "                    xy=(rect.get_x() + rect.get_width()/2, height),\n",
    "                    xytext=(0, 5),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=14, fontweight='bold', color=color)\n",
    "\n",
    "autolabel(rects1, 'black')\n",
    "autolabel(rects2, 'black')\n",
    "\n",
    "# Legend\n",
    "ax.legend(\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1.02, 1),\n",
    "    borderaxespad=0,\n",
    "    frameon=False,\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "# üîπ Remove outer box (all spines)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show transparent plot\n",
    "plt.show()\n",
    "\n",
    "# üîπ If saving, keep transparency\n",
    "# plt.savefig(\"augmentation_counts.png\", dpi=300, transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape =X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7353176,
     "sourceId": 11714488,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
