{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# üîÅ Full Reproducibility + Warning Suppression Setup\n",
    "# ==================================================\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# === ENVIRONMENT VARIABLES (SET BEFORE TF IMPORT) ===\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)           # Hash seed\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'           # Suppress TensorFlow logs\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '0'           # Allow non-deterministic ops to avoid UnimplementedError\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'           # Set GPU ID (or \"\" to force CPU)\n",
    "\n",
    "# === PYTHON SEED SETTINGS ===\n",
    "random.seed(SEED)\n",
    "\n",
    "# === SUPPRESS WARNINGS & LOGGING ===\n",
    "logging.getLogger('absl').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# === IMPORT LIBRARIES AFTER SEED SETTINGS ===\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === NUMPY & TENSORFLOW SEEDS ===\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# === SINGLE-THREADING FOR FULL REPRODUCIBILITY ===\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "# ‚úÖ CHECK GPU AVAILABILITY\n",
    "print(\"‚úÖ GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# ‚úÖ DATASET PATH\n",
    "DATASET_PATH = r'G:\\498R\\BanglaSER'\n",
    "\n",
    "# ‚úÖ EMOTION LABELS FROM FILENAME\n",
    "# Format: Mode-StatementType-Emotion-Intensity-Statement-Repetition-Actor.wav\n",
    "EMOTION_MAPPING = {\n",
    "    '01': 'happy',\n",
    "    '02': 'sad',\n",
    "    '03': 'angry',\n",
    "    '04': 'surprise',\n",
    "    '05': 'neutral'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hyperparameter Tuning<h1/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# ===============================\n",
    "# üîß Configuration\n",
    "# ===============================\n",
    "SAVE_PATH = r\"D:\\498R\"   # Adjust path\n",
    "batch_size = 32\n",
    "# ===============================\n",
    "# üìÇ Load datasets from disk\n",
    "# ===============================\n",
    "def load_dataset(filename):\n",
    "    path = os.path.join(SAVE_PATH, filename)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    data = np.load(path)\n",
    "    X, y = data['X'], data['y']\n",
    "    # Ensure correct dtype for TensorFlow\n",
    "    X = X.astype('float32')\n",
    "    y = y.astype('int64')\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = load_dataset(\"train(1000)(Cleaned).npz\")\n",
    "X_val, y_val     = load_dataset(\"val(1000)(Cleaned).npz\")\n",
    "X_test, y_test   = load_dataset(\"test(1000)(Cleaned).npz\")\n",
    "\n",
    "# ===============================\n",
    "# üîÅ Create tf.data pipelines\n",
    "# ===============================\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)) \\\n",
    "    .shuffle(buffer_size=len(X_train), seed=42) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)) \\\n",
    "    .batch(batch_size) \\\n",
    "    .cache() \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)) \\\n",
    "    .batch(batch_size) \\\n",
    "    .cache() \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ===============================\n",
    "# üõ† Sanity checks\n",
    "# ===============================\n",
    "print(f\"Train: {X_train.shape} {y_train.shape}\")\n",
    "print(f\"Val:   {X_val.shape} {y_val.shape}\")\n",
    "print(f\"Test:  {X_test.shape} {y_test.shape}\")\n",
    "\n",
    "print(\"Train classes:\", np.unique(y_train))\n",
    "print(\"Val classes:  \", np.unique(y_val))\n",
    "print(\"Test classes: \", np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, math, random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "EPOCHS = 20                     # you set this to 20 in your latest code\n",
    "SAVE_PATH = r\"D:\\498R\"\n",
    "SEED = 42\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Repro\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Optional: load datasets if not already present in memory\n",
    "# -----------------------------\n",
    "def _load_npz_dataset(base_path, fname):\n",
    "    path = os.path.join(base_path, fname)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    d = np.load(path)\n",
    "    X = d['X'].astype('float32')\n",
    "    y = d['y'].astype('int64')\n",
    "    return X, y\n",
    "\n",
    "if 'X_train' not in globals():\n",
    "    X_train, y_train = _load_npz_dataset(SAVE_PATH, \"train(1000)(Cleaned).npz\")\n",
    "    X_val,   y_val   = _load_npz_dataset(SAVE_PATH, \"val(1000)(Cleaned).npz\")\n",
    "    X_test,  y_test  = _load_npz_dataset(SAVE_PATH, \"test(1000)(Cleaned).npz\")\n",
    "\n",
    "print(f\"Train: {X_train.shape} {y_train.shape}\")\n",
    "print(f\"Val:   {X_val.shape} {y_val.shape}\")\n",
    "print(f\"Test:  {X_test.shape} {y_test.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Shapes & labels\n",
    "# -----------------------------\n",
    "input_shape = X_train.shape[1:]         # e.g., (126, 155)\n",
    "num_labels  = int(np.max([y_train.max(), y_val.max(), y_test.max()])) + 1\n",
    "\n",
    "# -----------------------------\n",
    "# Model (logits out; use from_logits=True)\n",
    "# -----------------------------\n",
    "def create_cnn_model(input_shape, num_labels):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(128, 5, padding='same', activation='relu')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=SEED)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(256, 5, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=SEED)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(512, 3, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=SEED)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(1024, 3, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=SEED)(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.4, seed=SEED)(x)\n",
    "\n",
    "    # logits (no softmax)\n",
    "    outputs = tf.keras.layers.Dense(num_labels)(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset builder per batch size\n",
    "# -----------------------------\n",
    "def make_tf_datasets(Xtr, ytr, Xva, yva, Xte, yte, batch_size):\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((Xtr, ytr)) \\\n",
    "        .shuffle(len(Xtr), seed=SEED) \\\n",
    "        .batch(batch_size) \\\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds   = tf.data.Dataset.from_tensor_slices((Xva, yva)) \\\n",
    "        .batch(batch_size) \\\n",
    "        .cache() \\\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds  = tf.data.Dataset.from_tensor_slices((Xte, yte)) \\\n",
    "        .batch(batch_size) \\\n",
    "        .cache() \\\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "# -----------------------------\n",
    "# Optimizer / Scheduler helpers\n",
    "# -----------------------------\n",
    "def get_optimizer(name, lr):\n",
    "    name = name.lower()\n",
    "    if name == 'adam':\n",
    "        return tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    if name == 'rmsprop':\n",
    "        return tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    if name == 'sgd':\n",
    "        return tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9, nesterov=True)\n",
    "    raise ValueError(f\"Unknown optimizer: {name}\")\n",
    "\n",
    "def get_scheduler_and_lr(sched_name, base_lr, steps_per_epoch, epochs):\n",
    "    \"\"\"\n",
    "    Returns (learning_rate_for_optimizer, callbacks_list)\n",
    "    - 'cosine': tf.keras CosineDecay schedule (no callback)\n",
    "    - 'step': LearningRateScheduler (half at halfway)\n",
    "    - 'plateau': ReduceLROnPlateau on val_loss\n",
    "    \"\"\"\n",
    "    sched_name = sched_name.lower()\n",
    "    if sched_name == 'cosine':\n",
    "        decay_steps = max(1, steps_per_epoch * epochs)\n",
    "        lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "            initial_learning_rate=base_lr, decay_steps=decay_steps\n",
    "        )\n",
    "        return lr_schedule, []\n",
    "    elif sched_name == 'step':\n",
    "        def step_fn(epoch, lr):\n",
    "            return base_lr * 0.5 if epoch >= (epochs // 2) else base_lr\n",
    "        cb = tf.keras.callbacks.LearningRateScheduler(step_fn, verbose=0)\n",
    "        return base_lr, [cb]\n",
    "    elif sched_name == 'plateau':\n",
    "        cb = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=0\n",
    "        )\n",
    "        return base_lr, [cb]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scheduler: {sched_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation on val set\n",
    "# -----------------------------\n",
    "def eval_on_val(model, val_ds):\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb in val_ds:\n",
    "        logits = model.predict(xb, verbose=0)\n",
    "        y_true.append(yb.numpy())\n",
    "        y_pred.append(np.argmax(logits, axis=1))\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return f1, acc\n",
    "\n",
    "# -----------------------------\n",
    "# Hyperparameter grid\n",
    "# -----------------------------\n",
    "OPTIMIZERS = ['Adam', 'RMSprop', 'SGD']\n",
    "BATCH_SIZES = [16, 32, 64]\n",
    "SCHEDULERS = ['Cosine', 'Step', 'Plateau']\n",
    "LRS = [1e-2, 2e-3, 1e-4, 4e-4, 1e-3]   # keep this order\n",
    "\n",
    "# -----------------------------\n",
    "# Run grid search\n",
    "# -----------------------------\n",
    "results = []\n",
    "total_runs = len(OPTIMIZERS)*len(BATCH_SIZES)*len(SCHEDULERS)*len(LRS)\n",
    "run_idx = 0\n",
    "\n",
    "for opt_name in OPTIMIZERS:\n",
    "    for bs in BATCH_SIZES:\n",
    "        train_ds, val_ds, test_ds = make_tf_datasets(X_train, y_train, X_val, y_val, X_test, y_test, bs)\n",
    "        steps_per_epoch = math.ceil(len(X_train)/bs)\n",
    "        for sched_name in SCHEDULERS:\n",
    "            for lr in LRS:\n",
    "                run_idx += 1\n",
    "                tf.keras.backend.clear_session()\n",
    "                model = create_cnn_model(input_shape, num_labels)\n",
    "\n",
    "                # compile with optimizer + scheduler; logits => from_logits=True\n",
    "                lr_or_schedule, sched_cbs = get_scheduler_and_lr(sched_name, lr, steps_per_epoch, EPOCHS)\n",
    "                optimizer = get_optimizer(opt_name, lr_or_schedule)\n",
    "                loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "                model.compile(optimizer=optimizer,\n",
    "                              loss=loss_obj,\n",
    "                              metrics=['accuracy'])\n",
    "\n",
    "                # train (fixed epochs; no early stopping to keep runs comparable)\n",
    "                start = time.time()\n",
    "                history = model.fit(\n",
    "                    train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=0,\n",
    "                    callbacks=sched_cbs\n",
    "                )\n",
    "                train_time = time.time() - start\n",
    "\n",
    "                # evaluate on val (macro-F1 + accuracy)\n",
    "                val_f1, val_acc = eval_on_val(model, val_ds)\n",
    "\n",
    "                results.append({\n",
    "                    \"optimizer\": opt_name,\n",
    "                    \"batch_size\": bs,\n",
    "                    \"scheduler\": sched_name,\n",
    "                    \"lr\": lr,\n",
    "                    \"val_macro_f1\": val_f1,\n",
    "                    \"val_accuracy\": val_acc,\n",
    "                    \"train_time_sec\": train_time,\n",
    "                    \"final_val_loss\": float(history.history['val_loss'][-1]),\n",
    "                    \"final_val_acc\":  float(history.history['val_accuracy'][-1]),\n",
    "                })\n",
    "                print(f\"[{run_idx:3d}/{total_runs}] opt={opt_name:7s} bs={bs:2d} \"\n",
    "                      f\"sch={sched_name:7s} lr={lr:.0e}  F1={val_f1:.4f}  Acc={val_acc:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Results as DataFrame + CSV\n",
    "# -----------------------------\n",
    "df = pd.DataFrame(results).sort_values(by=['val_macro_f1','val_accuracy'], ascending=False)\n",
    "csv_path = os.path.join(SAVE_PATH, \"gridsearch_results.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nSaved results to: {csv_path}\")\n",
    "print(df.head(10))\n",
    "\n",
    "# -----------------------------\n",
    "# Plots\n",
    "# -----------------------------\n",
    "def grouped_bar(ax, labels, vals1, vals2, legend1, legend2, title, ylabel):\n",
    "    x = np.arange(len(labels))\n",
    "    w = 0.35\n",
    "    ax.bar(x - w/2, vals1, width=w, label=legend1)\n",
    "    ax.bar(x + w/2, vals2, width=w, label=legend2)\n",
    "    ax.set_xticks(x, labels)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "# 1) Optimizer bar chart (aggregate across other params: mean)\n",
    "agg_opt = df.groupby('optimizer')[['val_macro_f1','val_accuracy']].mean().reindex(OPTIMIZERS)\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "grouped_bar(ax,\n",
    "            labels=agg_opt.index.tolist(),\n",
    "            vals1=agg_opt['val_macro_f1'].values,\n",
    "            vals2=agg_opt['val_accuracy'].values,\n",
    "            legend1='Val Macro-F1', legend2='Val Acc',\n",
    "            title='Optimizer Comparison (mean across grid)',\n",
    "            ylabel='Score')\n",
    "plt.tight_layout()\n",
    "opt_png = os.path.join(SAVE_PATH, \"hp_optimizer_bar.png\")\n",
    "plt.savefig(opt_png, dpi=120); plt.close()\n",
    "print(f\"Saved: {opt_png}\")\n",
    "\n",
    "# 2) Scheduler bar chart (aggregate across other params: mean)\n",
    "agg_sch = df.groupby('scheduler')[['val_macro_f1','val_accuracy']].mean().reindex(SCHEDULERS)\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "grouped_bar(ax,\n",
    "            labels=agg_sch.index.tolist(),\n",
    "            vals1=agg_sch['val_macro_f1'].values,\n",
    "            vals2=agg_sch['val_accuracy'].values,\n",
    "            legend1='Val Macro-F1', legend2='Val Acc',\n",
    "            title='LR Scheduler Comparison (mean across grid)',\n",
    "            ylabel='Score')\n",
    "plt.tight_layout()\n",
    "sch_png = os.path.join(SAVE_PATH, \"hp_scheduler_bar.png\")\n",
    "plt.savefig(sch_png, dpi=120); plt.close()\n",
    "print(f\"Saved: {sch_png}\")\n",
    "\n",
    "# 3) LR sensitivity (aggregate: mean over other params)\n",
    "agg_lr = df.groupby('lr')[['val_macro_f1','val_accuracy']].mean().reindex(LRS)\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax.plot(agg_lr.index.values, agg_lr['val_macro_f1'].values, marker='o', label='Val Macro-F1')\n",
    "ax.plot(agg_lr.index.values, agg_lr['val_accuracy'].values, marker='o', label='Val Acc')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Learning Rate (log scale)')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Learning Rate Sensitivity (mean across grid)')\n",
    "ax.grid(True, which='both', linestyle='--', alpha=0.3)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "lr_png = os.path.join(SAVE_PATH, \"hp_lr_sensitivity.png\")\n",
    "plt.savefig(lr_png, dpi=120); plt.close()\n",
    "print(f\"Saved: {lr_png}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Print the best config\n",
    "# -----------------------------\n",
    "best = df.iloc[0]\n",
    "print(\"\\nBest config by Val Macro-F1:\")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Clean, padded plots (white bg, no grid, pastel palettes)\n",
    "# -----------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['savefig.facecolor'] = 'white'\n",
    "\n",
    "# pastel palettes (two colors per grouped bar fig, one pair per figure)\n",
    "PALETTE_OPT   = ('#D7A4A9', '#A86380')  # rose + plum\n",
    "PALETTE_SCHED = ('#9CC8B0', '#5E9C8B')  # mint + teal\n",
    "PALETTE_BS    = ('#BBB3E3', '#7C82B6')  # lavender + slate\n",
    "LINE_F1_COLOR  = '#E67E22'              # orange\n",
    "LINE_ACC_COLOR = '#2E8B57'              # sea green\n",
    "\n",
    "def _add_headroom(ax, top_pad=0.08, bottom_pad=0.02):\n",
    "    \"\"\"Give axes some headroom so labels don't touch borders.\"\"\"\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    rng = ymax - ymin if ymax > ymin else 1.0\n",
    "    ax.set_ylim(ymin - bottom_pad*rng, ymax + top_pad*rng)\n",
    "\n",
    "def _format_val(v):\n",
    "    # If scores look like 0‚Äì1, print as %; else keep 3 decimals.\n",
    "    return f\"{v*100:.1f}%\" if 0.0 <= v <= 1.5 else f\"{v:.3f}\"\n",
    "\n",
    "def add_bar_labels(ax, bars, fontsize=18, y_offset_frac=0.015):\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    rng = ymax - ymin if ymax > ymin else 1.0\n",
    "    for b in bars:\n",
    "        h = b.get_height()\n",
    "        ax.text(b.get_x() + b.get_width()/2,\n",
    "                h + y_offset_frac*rng,\n",
    "                _format_val(h),\n",
    "                ha='center', va='bottom', fontsize=fontsize, clip_on=False)\n",
    "\n",
    "def grouped_bar(ax, labels, vals1, vals2, legend1, legend2, title, ylabel,\n",
    "                colors=('C0','C1')):\n",
    "    x = np.arange(len(labels))\n",
    "    w = 0.36\n",
    "\n",
    "    b1 = ax.bar(x - w/2, vals1, width=w, label=legend1, color=colors[0])\n",
    "    b2 = ax.bar(x + w/2, vals2, width=w, label=legend2, color=colors[1])\n",
    "\n",
    "    ax.set_xticks(x, labels)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.set_facecolor('white')  # no grid / white bg\n",
    "    # padding + labels\n",
    "    _add_headroom(ax, top_pad=0.10)\n",
    "    add_bar_labels(ax, b1, fontsize=18)\n",
    "    add_bar_labels(ax, b2, fontsize=18)\n",
    "\n",
    "# 1) Optimizer bar chart (aggregate across other params: mean)\n",
    "agg_opt = df.groupby('optimizer')[['val_macro_f1','val_accuracy']].mean().reindex(OPTIMIZERS)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "grouped_bar(ax,\n",
    "            labels=agg_opt.index.tolist(),\n",
    "            vals1=agg_opt['val_macro_f1'].values,\n",
    "            vals2=agg_opt['val_accuracy'].values,\n",
    "            legend1='Val Macro-F1', legend2='Val Acc',\n",
    "            title='Optimizer Comparison (mean across grid)',\n",
    "            ylabel='Score',\n",
    "            colors=PALETTE_OPT)\n",
    "plt.tight_layout()\n",
    "opt_png = os.path.join(SAVE_PATH, \"hp_optimizer_bar.png\")\n",
    "plt.savefig(opt_png, dpi=150, bbox_inches='tight', pad_inches=0.25); plt.close()\n",
    "print(f\"Saved: {opt_png}\")\n",
    "\n",
    "# 2) Scheduler bar chart (aggregate across other params: mean)\n",
    "agg_sch = df.groupby('scheduler')[['val_macro_f1','val_accuracy']].mean().reindex(SCHEDULERS)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "grouped_bar(ax,\n",
    "            labels=agg_sch.index.tolist(),\n",
    "            vals1=agg_sch['val_macro_f1'].values,\n",
    "            vals2=agg_sch['val_accuracy'].values,\n",
    "            legend1='Val Macro-F1', legend2='Val Acc',\n",
    "            title='LR Scheduler Comparison (mean across grid)',\n",
    "            ylabel='Score',\n",
    "            colors=PALETTE_SCHED)\n",
    "plt.tight_layout()\n",
    "sch_png = os.path.join(SAVE_PATH, \"hp_scheduler_bar.png\")\n",
    "plt.savefig(sch_png, dpi=150, bbox_inches='tight', pad_inches=0.25); plt.close()\n",
    "print(f\"Saved: {sch_png}\")\n",
    "\n",
    "# 3) Batch size bar chart (aggregate across other params: mean)\n",
    "agg_bs = df.groupby('batch_size')[['val_macro_f1','val_accuracy']].mean().reindex(BATCH_SIZES)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "grouped_bar(ax,\n",
    "            labels=[str(x) for x in agg_bs.index.tolist()],\n",
    "            vals1=agg_bs['val_macro_f1'].values,\n",
    "            vals2=agg_bs['val_accuracy'].values,\n",
    "            legend1='Val Macro-F1', legend2='Val Acc',\n",
    "            title='Batch Size Comparison (mean across grid)',\n",
    "            ylabel='Score',\n",
    "            colors=PALETTE_BS)\n",
    "plt.tight_layout()\n",
    "bs_png = os.path.join(SAVE_PATH, \"hp_batchsize_bar.png\")\n",
    "plt.savefig(bs_png, dpi=150, bbox_inches='tight', pad_inches=0.25); plt.close()\n",
    "print(f\"Saved: {bs_png}\")\n",
    "\n",
    "# 4) LR sensitivity (aggregate: mean over other params) with padded labels\n",
    "agg_lr = df.groupby('lr')[['val_macro_f1','val_accuracy']].mean().reindex(LRS)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "x_lr = np.array(agg_lr.index.values, dtype=float)\n",
    "y_f1 = agg_lr['val_macro_f1'].values\n",
    "y_acc = agg_lr['val_accuracy'].values\n",
    "\n",
    "l1, = ax.plot(x_lr, y_f1, marker='o', linewidth=2.5, markersize=8,\n",
    "              label='Val Macro-F1', color=LINE_F1_COLOR)\n",
    "l2, = ax.plot(x_lr, y_acc, marker='o', linewidth=2.5, markersize=8,\n",
    "              label='Val Acc', color=LINE_ACC_COLOR)\n",
    "\n",
    "# add a little headroom and per-point labels\n",
    "_add_headroom(ax, top_pad=0.12)\n",
    "ymin, ymax = ax.get_ylim(); rng = ymax - ymin if ymax > ymin else 1.0\n",
    "for xv, yv in zip(x_lr, y_f1):\n",
    "    ax.text(xv, yv + 0.015*rng, _format_val(yv), ha='center', va='bottom', fontsize=18, clip_on=False)\n",
    "for xv, yv in zip(x_lr, y_acc):\n",
    "    ax.text(xv, yv + 0.015*rng, _format_val(yv), ha='center', va='bottom', fontsize=18, clip_on=False)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Learning Rate (log scale)')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Learning Rate Sensitivity (mean across grid)')\n",
    "ax.legend()\n",
    "ax.set_facecolor('white')\n",
    "plt.tight_layout()\n",
    "lr_png = os.path.join(SAVE_PATH, \"hp_lr_sensitivity.png\")\n",
    "plt.savefig(lr_png, dpi=150, bbox_inches='tight', pad_inches=0.25); plt.close()\n",
    "print(f\"Saved: {lr_png}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "CSV_PATH = r\"G:\\498R\\gridsearch_results.csv\"\n",
    "OUT_DIR  = os.path.dirname(CSV_PATH)\n",
    "\n",
    "# -----------------------------\n",
    "# Load results\n",
    "# -----------------------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Desired order (only keep levels that actually exist in the CSV)\n",
    "OPTIMIZERS = [o for o in ['Adam','RMSprop','SGD'] if o in set(df['optimizer'])]\n",
    "SCHEDULERS = [s for s in ['Cosine','Step','Plateau'] if s in set(df['scheduler'])]\n",
    "BATCH_SIZES = [b for b in [16,32,64] if b in set(df['batch_size'])]\n",
    "LRS = [lr for lr in [1e-2, 2e-3, 1e-4, 4e-4, 1e-3] if lr in set(df['lr'])]\n",
    "\n",
    "# -----------------------------\n",
    "# Colors (same as before)\n",
    "# -----------------------------\n",
    "PALETTE_OPT   = ('#D7A4A9', '#A86380')  # rose + plum\n",
    "PALETTE_SCHED = ('#9CC8B0', '#5E9C8B')  # mint + teal\n",
    "PALETTE_BS    = ('#BBB3E3', '#7C82B6')  # lavender + slate\n",
    "LINE_F1_COLOR  = '#E67E22'              # orange\n",
    "LINE_ACC_COLOR = '#2E8B57'              # sea green\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers (transparent, no grid, no value text)\n",
    "# -----------------------------\n",
    "def grouped_bar_transparent(filename, labels, vals1, vals2, legend1, legend2, title, ylabel, colors):\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    fig.patch.set_alpha(0)           # transparent figure\n",
    "    ax.set_facecolor('none')         # transparent axes\n",
    "    x = np.arange(len(labels)); w = 0.36\n",
    "    ax.bar(x - w/2, vals1, width=w, label=legend1, color=colors[0])\n",
    "    ax.bar(x + w/2, vals2, width=w, label=legend2, color=colors[1])\n",
    "    ax.set_xticks(x, labels)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    # no grid; small headroom so bars don't touch frame\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    rng = (ymax - ymin) if ymax > ymin else 1.0\n",
    "    ax.set_ylim(ymin, ymax + 0.05*rng)\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(OUT_DIR, filename)\n",
    "    plt.savefig(out_path, dpi=150, bbox_inches='tight', pad_inches=0.25, transparent=True)\n",
    "    plt.close()\n",
    "    print(f\"Saved: {out_path}\")\n",
    "\n",
    "def lr_line_transparent(filename, x_vals, y_f1, y_acc, title):\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    fig.patch.set_alpha(0)\n",
    "    ax.set_facecolor('none')\n",
    "    ax.plot(x_vals, y_f1, marker='o', linewidth=2.5, markersize=8, label='Val Macro-F1', color=LINE_F1_COLOR)\n",
    "    ax.plot(x_vals, y_acc, marker='o', linewidth=2.5, markersize=8, label='Val Acc', color=LINE_ACC_COLOR)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Learning Rate (log scale)')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    # small headroom; no grid\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    rng = (ymax - ymin) if ymax > ymin else 1.0\n",
    "    ax.set_ylim(ymin, ymax + 0.06*rng)\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(OUT_DIR, filename)\n",
    "    plt.savefig(out_path, dpi=150, bbox_inches='tight', pad_inches=0.25, transparent=True)\n",
    "    plt.close()\n",
    "    print(f\"Saved: {out_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Aggregations (means across other params)\n",
    "# -----------------------------\n",
    "agg_opt = df.groupby('optimizer')[['val_macro_f1','val_accuracy']].mean().reindex(OPTIMIZERS)\n",
    "grouped_bar_transparent(\n",
    "    filename=\"hp_optimizer_bar.png\",\n",
    "    labels=agg_opt.index.tolist(),\n",
    "    vals1=agg_opt['val_macro_f1'].values,\n",
    "    vals2=agg_opt['val_accuracy'].values,\n",
    "    legend1='Val Macro-F1', legend2='Val Acc',\n",
    "    title='Optimizer Comparison (mean across grid)',\n",
    "    ylabel='Score',\n",
    "    colors=PALETTE_OPT\n",
    ")\n",
    "\n",
    "agg_sch = df.groupby('scheduler')[['val_macro_f1','val_accuracy']].mean().reindex(SCHEDULERS)\n",
    "grouped_bar_transparent(\n",
    "    filename=\"hp_scheduler_bar.png\",\n",
    "    labels=agg_sch.index.tolist(),\n",
    "    vals1=agg_sch['val_macro_f1'].values,\n",
    "    vals2=agg_sch['val_accuracy'].values,\n",
    "    legend1='Val Macro-F1', legend2='Val Acc',\n",
    "    title='LR Scheduler Comparison (mean across grid)',\n",
    "    ylabel='Score',\n",
    "    colors=PALETTE_SCHED\n",
    ")\n",
    "\n",
    "agg_bs = df.groupby('batch_size')[['val_macro_f1','val_accuracy']].mean().reindex(BATCH_SIZES)\n",
    "grouped_bar_transparent(\n",
    "    filename=\"hp_batchsize_bar.png\",\n",
    "    labels=[str(x) for x in agg_bs.index.tolist()],\n",
    "    vals1=agg_bs['val_macro_f1'].values,\n",
    "    vals2=agg_bs['val_accuracy'].values,\n",
    "    legend1='Val Macro-F1', legend2='Val Acc',\n",
    "    title='Batch Size Comparison (mean across grid)',\n",
    "    ylabel='Score',\n",
    "    colors=PALETTE_BS\n",
    ")\n",
    "\n",
    "agg_lr = df.groupby('lr')[['val_macro_f1','val_accuracy']].mean()\n",
    "# keep only the LRs present and in desired order\n",
    "agg_lr = agg_lr.loc[[lr for lr in LRS if lr in agg_lr.index]]\n",
    "lr_line_transparent(\n",
    "    filename=\"hp_lr_sensitivity.png\",\n",
    "    x_vals=np.array(agg_lr.index.values, dtype=float),\n",
    "    y_f1=agg_lr['val_macro_f1'].values,\n",
    "    y_acc=agg_lr['val_accuracy'].values,\n",
    "    title='Learning Rate Sensitivity (mean across grid)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7353176,
     "sourceId": 11714488,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
