{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# ðŸ” Full Reproducibility + Warning Suppression Setup\n",
    "# ==================================================\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# === ENVIRONMENT VARIABLES (SET BEFORE TF IMPORT) ===\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)           # Hash seed\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'           # Suppress TensorFlow logs\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '0'           # Allow non-deterministic ops to avoid UnimplementedError\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'           # Set GPU ID (or \"\" to force CPU)\n",
    "\n",
    "# === PYTHON SEED SETTINGS ===\n",
    "random.seed(SEED)\n",
    "\n",
    "# === SUPPRESS WARNINGS & LOGGING ===\n",
    "logging.getLogger('absl').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# === IMPORT LIBRARIES AFTER SEED SETTINGS ===\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === NUMPY & TENSORFLOW SEEDS ===\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# === SINGLE-THREADING FOR FULL REPRODUCIBILITY ===\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "# âœ… CHECK GPU AVAILABILITY\n",
    "print(\"âœ… GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# âœ… DATASET PATH\n",
    "DATASET_PATH = r'G:\\498R\\BanglaSER'\n",
    "\n",
    "# âœ… EMOTION LABELS FROM FILENAME\n",
    "# Format: Mode-StatementType-Emotion-Intensity-Statement-Repetition-Actor.wav\n",
    "EMOTION_MAPPING = {\n",
    "    '01': 'happy',\n",
    "    '02': 'sad',\n",
    "    '03': 'angry',\n",
    "    '04': 'surprise',\n",
    "    '05': 'neutral'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset information from file structure\n",
    "def create_dataset_info(root_path):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                # Parse the filename to get emotion\n",
    "                # Format: Mode-StatementType-Emotion-Intensity-Statement-Repetition-Actor.wav\n",
    "                parts = file.split('-')\n",
    "                if len(parts) == 7:\n",
    "                    emotion_code = parts[2]\n",
    "                    emotion = EMOTION_MAPPING.get(emotion_code, 'unknown')\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    data.append({\n",
    "                        'file_path': file_path,\n",
    "                        'emotion': emotion,\n",
    "                        'emotion_code': emotion_code\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create and display dataset info\n",
    "df = create_dataset_info(DATASET_PATH)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(df['emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# SER model comparison on one dataset (YAMNet vs VGGish)\n",
    "# Your duplicate-check logic applied:\n",
    "#   - Global de-dup on RAW AUDIO (after augmentation) BEFORE split\n",
    "#   - Post-split duplicate detection/cleaning + GRAPHS (your block)\n",
    "# ==================================================\n",
    "import os, random, warnings, logging, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger('absl').setLevel(logging.ERROR)\n",
    "\n",
    "# ---------------------------\n",
    "# Config (EDIT THESE)\n",
    "# ---------------------------\n",
    "SEED = 42\n",
    "DATASET_PATH = r'G:\\498R\\BanglaSER'       # folder with .wav\n",
    "SAVE_PATH    = r'G:\\498R\\exp_outputs'     # where to store npz, figs, results\n",
    "FEATURE_TYPE = \"yamnet\"                   # \"yamnet\" or \"vggish\"\n",
    "TARGET_PER_CLASS = 1000                   # augmentation target per class\n",
    "BATCH_SIZE = 32\n",
    "DURATION_SEC = 4.0\n",
    "TARGET_SR = 16000\n",
    "FIXED_FRAMES = 8                          # ~8 frames for ~4 s clips\n",
    "RUN_DUP_CHECK_PLOTS = True\n",
    "\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset discovery + labels\n",
    "# ---------------------------\n",
    "# Format: Mode-StatementType-Emotion-Intensity-Statement-Repetition-Actor.wav\n",
    "EMOTION_MAPPING = {'01':'happy','02':'sad','03':'angry','04':'surprise','05':'neutral'}\n",
    "\n",
    "def create_dataset_info(root_path):\n",
    "    rows = []\n",
    "    for root, _, files in os.walk(root_path):\n",
    "        for f in files:\n",
    "            if f.lower().endswith('.wav'):\n",
    "                parts = f.split('-')\n",
    "                if len(parts) == 7:\n",
    "                    code = parts[2]\n",
    "                    emotion = EMOTION_MAPPING.get(code, 'unknown')\n",
    "                    rows.append({'file_path': os.path.join(root, f),\n",
    "                                 'emotion': emotion,\n",
    "                                 'emotion_code': code})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = create_dataset_info(DATASET_PATH)\n",
    "assert len(df) > 0, \"No .wav files found.\"\n",
    "emotions = sorted(df['emotion'].unique())\n",
    "label_mapping = {lab:i for i,lab in enumerate(emotions)}\n",
    "reverse_mapping = {i:lab for lab,i in label_mapping.items()}\n",
    "num_labels = len(emotions)\n",
    "print(\"Label map:\", label_mapping)\n",
    "\n",
    "# ---------------------------\n",
    "# Audio IO + augmentation\n",
    "# ---------------------------\n",
    "TARGET_LEN = int(TARGET_SR * DURATION_SEC)\n",
    "\n",
    "def load_audio_fixed(path, sr=TARGET_SR, duration=DURATION_SEC):\n",
    "    y, sr = librosa.load(path, sr=sr, duration=duration, mono=True)\n",
    "    if len(y) < TARGET_LEN: y = np.pad(y, (0, TARGET_LEN - len(y)))\n",
    "    else:                   y = y[:TARGET_LEN]\n",
    "    return y.astype(np.float32, copy=False), sr\n",
    "\n",
    "def ensure_length(y, L=TARGET_LEN):\n",
    "    if len(y) < L: y = np.pad(y, (0, L-len(y)))\n",
    "    elif len(y) > L: y = y[:L]\n",
    "    return y.astype(np.float32, copy=False)\n",
    "\n",
    "def aug_noise(y, std=0.005): return (y + np.random.normal(0.0, std, size=y.shape)).astype(np.float32)\n",
    "def aug_shift(y, max_ratio=0.1):\n",
    "    m = int(len(y)*max_ratio); s = np.random.randint(-m, m+1)\n",
    "    if s==0: return y\n",
    "    if s>0:  return np.concatenate([np.zeros(s, y.dtype), y[:-s]]).astype(np.float32)\n",
    "    s=-s;    return np.concatenate([y[s:], np.zeros(s, y.dtype)]).astype(np.float32)\n",
    "def aug_pitch(y, sr=TARGET_SR, rng=(-2,2)):\n",
    "    steps = np.random.uniform(rng[0], rng[1])\n",
    "    z = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=steps)\n",
    "    return ensure_length(z, TARGET_LEN)\n",
    "def aug_stretch(y, rng=(0.8,1.25)):\n",
    "    r = np.random.uniform(rng[0], rng[1])\n",
    "    z = librosa.effects.time_stretch(y=y, rate=r)\n",
    "    return ensure_length(z, TARGET_LEN)\n",
    "\n",
    "AUG_FUNCS = ['noise','shift','pitch','stretch']\n",
    "def apply_aug(y, m):\n",
    "    return aug_noise(y) if m=='noise' else aug_shift(y) if m=='shift' else aug_pitch(y) if m=='pitch' else aug_stretch(y)\n",
    "\n",
    "# ---------------------------\n",
    "# YAMNet/VGGish feature extractors\n",
    "# ---------------------------\n",
    "YAMNET_HANDLE = \"https://tfhub.dev/google/yamnet/1\"\n",
    "VGGISH_HANDLE = \"https://tfhub.dev/google/vggish/1\"\n",
    "\n",
    "print(\"Loading TF-Hub modelsâ€¦\")\n",
    "yamnet_model = hub.load(YAMNET_HANDLE)\n",
    "vggish_model = hub.load(VGGISH_HANDLE)\n",
    "\n",
    "YAMNET_DIM, VGGISH_DIM = 1024, 128\n",
    "\n",
    "def pad_trunc_frames(X, T=FIXED_FRAMES, D=None):\n",
    "    # X: (n, d)\n",
    "    if X is None or X.size == 0:\n",
    "        d = (X.shape[1] if (X is not None and X.ndim==2 and X.size>0) else (D if D is not None else YAMNET_DIM))\n",
    "        return np.zeros((T, d), dtype=np.float32)\n",
    "    n, d = X.shape\n",
    "    if n >= T: return X[:T]\n",
    "    out = np.zeros((T,d), dtype=X.dtype)\n",
    "    out[:n] = X\n",
    "    return out\n",
    "\n",
    "def yamnet_embed_frames(y, sr=TARGET_SR):\n",
    "    if sr != 16000:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=16000)\n",
    "    w = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "    _, e, _ = yamnet_model(w)          # (n,1024)\n",
    "    e = e.numpy().astype(np.float32)\n",
    "    return pad_trunc_frames(e, T=FIXED_FRAMES, D=YAMNET_DIM)\n",
    "\n",
    "def vggish_embed_frames(y, sr=TARGET_SR):\n",
    "    if sr != 16000:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=16000)\n",
    "    w = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "    e = vggish_model(w).numpy().astype(np.float32)  # (n,128)\n",
    "    return pad_trunc_frames(e, T=FIXED_FRAMES, D=VGGISH_DIM)\n",
    "\n",
    "def extract_fn_for(feature_type):\n",
    "    return yamnet_embed_frames if feature_type.lower()==\"yamnet\" else vggish_embed_frames\n",
    "\n",
    "# ==================================================\n",
    "# >>> Global augmentation, then GLOBAL de-dup on RAW AUDIO (your hashing approach) <<<\n",
    "# ==================================================\n",
    "def hash_sample_audio(arr, decimals=6):\n",
    "    \"\"\"Robust hash for 1D float audio arrays.\"\"\"\n",
    "    a = np.asarray(arr, dtype=np.float32)\n",
    "    if a.ndim == 0:\n",
    "        a = a.reshape(1)\n",
    "    a = np.round(a, decimals=decimals)\n",
    "    a = np.ascontiguousarray(a)\n",
    "    return hashlib.sha1(a.tobytes()).hexdigest()\n",
    "\n",
    "def build_augmented_audio_dataset(df, target_per_class=1000):\n",
    "    \"\"\"Return raw audio clips AFTER augmentation + global de-dup (audio-level), with labels.\"\"\"\n",
    "    raw_audio_list = []   # list of 1D float32 arrays (len = TARGET_LEN)\n",
    "    labels_list    = []   # int labels\n",
    "    by_class = defaultdict(list)\n",
    "\n",
    "    # originals\n",
    "    print(\"\\nLoading base audio â€¦\")\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        y, sr = load_audio_fixed(row['file_path'], sr=TARGET_SR, duration=DURATION_SEC)\n",
    "        lab = label_mapping[row['emotion']]\n",
    "        raw_audio_list.append(y.astype(np.float32, copy=False))\n",
    "        labels_list.append(lab)\n",
    "        by_class[lab].append(y.astype(np.float32, copy=False))\n",
    "\n",
    "    # augment to balance\n",
    "    print(\"Augmenting to balance â€¦\")\n",
    "    for lab, auds in by_class.items():\n",
    "        need = max(0, target_per_class - len(auds))\n",
    "        i = 0\n",
    "        while need > 0:\n",
    "            base = auds[i % len(auds)]\n",
    "            m = AUG_FUNCS[np.random.randint(0, len(AUG_FUNCS))]\n",
    "            ya = apply_aug(base, m)\n",
    "            ya = ensure_length(ya, TARGET_LEN).astype(np.float32, copy=False)\n",
    "            raw_audio_list.append(ya)\n",
    "            labels_list.append(lab)\n",
    "            i += 1\n",
    "            need -= 1\n",
    "\n",
    "    # Keep as list for hashing; labels as ndarray\n",
    "    labels = np.array(labels_list, dtype=np.int32)\n",
    "\n",
    "    print(\"Global audio de-dup (before split) â€¦\")\n",
    "    hashes = [hash_sample_audio(x) for x in raw_audio_list]\n",
    "\n",
    "    seen, keep_idx = set(), []\n",
    "    for i, h in enumerate(hashes):\n",
    "        if h not in seen:\n",
    "            seen.add(h)\n",
    "            keep_idx.append(i)\n",
    "    keep_idx = np.array(keep_idx, dtype=int)\n",
    "\n",
    "    raw_audio_clean = [raw_audio_list[i] for i in keep_idx]\n",
    "    labels_clean    = labels[keep_idx]\n",
    "\n",
    "    print(f\"Kept {len(raw_audio_clean)} / {len(raw_audio_list)} after global audio de-dup.\")\n",
    "\n",
    "    return raw_audio_clean, labels_clean\n",
    "\n",
    "# ==================================================\n",
    "# Split once (shared by both models)\n",
    "# ==================================================\n",
    "SPLITS_FILE = os.path.join(SAVE_PATH, \"splits_indices_audiohash.npz\")\n",
    "\n",
    "def get_or_make_splits_shared(labels_clean):\n",
    "    if os.path.exists(SPLITS_FILE):\n",
    "        s = np.load(SPLITS_FILE)\n",
    "        return s['train_idx'], s['val_idx'], s['test_idx']\n",
    "    idx = np.arange(len(labels_clean))\n",
    "    tr, tmp, y_tr, y_tmp = train_test_split(idx, labels_clean, test_size=0.2, random_state=SEED, stratify=labels_clean)\n",
    "    va, te, _, _ = train_test_split(tmp, y_tmp, test_size=0.5, random_state=SEED, stratify=y_tmp)\n",
    "    np.savez_compressed(SPLITS_FILE, train_idx=tr, val_idx=va, test_idx=te)\n",
    "    return tr, va, te\n",
    "\n",
    "# ==================================================\n",
    "# Build embeddings for the chosen feature type\n",
    "# ==================================================\n",
    "def audio_to_embeddings(audio_list, feature_type=\"yamnet\"):\n",
    "    fn = extract_fn_for(feature_type)\n",
    "    X = []\n",
    "    for y in tqdm(audio_list, desc=f\"Embedding {feature_type}\"):\n",
    "        X.append(fn(y, TARGET_SR))  # (T,F)\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    return X\n",
    "\n",
    "# ==================================================\n",
    "# Scaling + datasets\n",
    "# ==================================================\n",
    "def scale_and_make_datasets(X_train, X_val, X_test, y_train, y_val, y_test, batch=BATCH_SIZE):\n",
    "    T, F = X_train.shape[1], X_train.shape[2]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train.reshape(-1, F))\n",
    "\n",
    "    def tfm(X):\n",
    "        N = X.shape[0]\n",
    "        return scaler.transform(X.reshape(-1, F)).reshape(N, T, F).astype(np.float32)\n",
    "\n",
    "    Xtr, Xva, Xte = tfm(X_train), tfm(X_val), tfm(X_test)\n",
    "    y_train = y_train.astype(np.int32); y_val = y_val.astype(np.int32); y_test = y_test.astype(np.int32)\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((Xtr, y_train)).shuffle(len(Xtr), seed=SEED).batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds   = tf.data.Dataset.from_tensor_slices((Xva, y_val)).batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds  = tf.data.Dataset.from_tensor_slices((Xte, y_test)).batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    return Xtr, Xva, Xte, train_ds, val_ds, test_ds, scaler\n",
    "\n",
    "# ==================================================\n",
    "# Model + training + eval\n",
    "# ==================================================\n",
    "def build_bilstm_classifier(T, F, C):\n",
    "    inp = layers.Input(shape=(T, F))\n",
    "    x = layers.Masking()(inp)\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(C, activation='softmax')(x)\n",
    "    model = models.Model(inp, out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def callbacks_for(name):\n",
    "    return [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True, verbose=1),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(SAVE_PATH, f'best_{name}.h5'),\n",
    "                                           monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "    ]\n",
    "\n",
    "def train_and_eval(feature_type, X_train, X_val, X_test, y_train, y_val, y_test, train_ds, val_ds, test_ds):\n",
    "    T, F = X_train.shape[1], X_train.shape[2]\n",
    "    name = f\"{feature_type.upper()}_BiLSTM_T{T}_F{F}\"\n",
    "    model = build_bilstm_classifier(T, F, num_labels)\n",
    "\n",
    "    print(f\"\\nTraining {name} â€¦\")\n",
    "    hist = model.fit(train_ds, validation_data=val_ds, epochs=100, callbacks=callbacks_for(name), verbose=1)\n",
    "    model.load_weights(os.path.join(SAVE_PATH, f'best_{name}.h5'))\n",
    "\n",
    "    # Eval\n",
    "    eval_out = model.evaluate(test_ds, return_dict=True, verbose=1)\n",
    "    test_acc = float(eval_out.get('accuracy', np.nan))\n",
    "    y_prob = model.predict(test_ds, verbose=0)\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "    y_true = np.concatenate([b.numpy() for _, b in test_ds], axis=0)\n",
    "\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"Test acc: {test_acc:.4f} | Macro-F1: {macro_f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(\n",
    "        y_true, y_pred, target_names=[reverse_mapping[i] for i in range(num_labels)]\n",
    "    ))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=[reverse_mapping[i] for i in range(num_labels)],\n",
    "                yticklabels=[reverse_mapping[i] for i in range(num_labels)])\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(SAVE_PATH, f'{name}_cm.png')); plt.close()\n",
    "\n",
    "    # Curves\n",
    "    h = hist.history\n",
    "    plt.figure(); plt.plot(h['accuracy']); plt.plot(h['val_accuracy']); plt.title(f'{name} Accuracy'); plt.xlabel('epoch'); plt.ylabel('acc'); plt.legend(['train','val']); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_PATH, f'{name}_acc.png')); plt.close()\n",
    "    plt.figure(); plt.plot(h['loss']); plt.plot(h['val_loss']); plt.title(f'{name} Loss'); plt.xlabel('epoch'); plt.ylabel('loss'); plt.legend(['train','val']); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_PATH, f'{name}_loss.png')); plt.close()\n",
    "\n",
    "    # Save row\n",
    "    res_path = os.path.join(SAVE_PATH, \"results.csv\")\n",
    "    row = pd.DataFrame([{\"feature_type\": feature_type, \"T\": T, \"F\": F, \"test_accuracy\": test_acc, \"macro_f1\": macro_f1}])\n",
    "    if os.path.exists(res_path): row.to_csv(res_path, mode='a', header=False, index=False)\n",
    "    else:                        row.to_csv(res_path, index=False)\n",
    "    print(f\"Saved results to {res_path}\")\n",
    "    return test_acc, macro_f1\n",
    "\n",
    "# ==================================================\n",
    "# >>> Build raw audio dataset (augmented) and de-dup globally <<<\n",
    "# ==================================================\n",
    "raw_audio_clean, labels_clean = build_augmented_audio_dataset(df, target_per_class=TARGET_PER_CLASS)\n",
    "\n",
    "# ==================================================\n",
    "# >>> Get or create shared splits on the CLEANED audio dataset <<<\n",
    "# ==================================================\n",
    "train_idx, val_idx, test_idx = get_or_make_splits_shared(labels_clean)\n",
    "\n",
    "# ==================================================\n",
    "# >>> Convert cleaned audio to embeddings for chosen model, then slice splits <<<\n",
    "# ==================================================\n",
    "X_all = audio_to_embeddings([raw_audio_clean[i] for i in range(len(raw_audio_clean))],\n",
    "                            feature_type=FEATURE_TYPE)\n",
    "y_all = labels_clean.copy()\n",
    "\n",
    "X_train, y_train = X_all[train_idx], y_all[train_idx]\n",
    "X_val,   y_val   = X_all[val_idx],   y_all[val_idx]\n",
    "X_test,  y_test  = X_all[test_idx],  y_all[test_idx]\n",
    "\n",
    "# ==================================================\n",
    "# >>> Scale + make datasets (scaler fit on TRAIN only)\n",
    "# ==================================================\n",
    "X_train, X_val, X_test, train_ds, val_ds, test_ds, scaler = scale_and_make_datasets(\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, batch=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# ==================================================\n",
    "# >>> YOUR POST-SPLIT DUPLICATE DETECTION / CLEANING + GRAPHS (unchanged) <<<\n",
    "# ==================================================\n",
    "# snapshot \"before\" for graphs\n",
    "X_train_bef, y_train_bef = X_train.copy(), y_train.copy()\n",
    "X_val_bef,   y_val_bef   = X_val.copy(),   y_val.copy()\n",
    "X_test_bef,  y_test_bef  = X_test.copy(),  y_test.copy()\n",
    "\n",
    "def hash_sample(arr, decimals=6):\n",
    "    a = np.ascontiguousarray(np.round(arr, decimals))\n",
    "    return hashlib.sha1(a.view(np.uint8)).hexdigest()\n",
    "\n",
    "def hashes_for_split(X):\n",
    "    return np.array([hash_sample(x) for x in X])\n",
    "\n",
    "def find_within_split_dups(hashes):\n",
    "    buckets = defaultdict(list)\n",
    "    for i, h in enumerate(hashes):\n",
    "        buckets[h].append(i)\n",
    "    return {h: idxs for h, idxs in buckets.items() if len(idxs) > 1}\n",
    "\n",
    "def count_within_extra(dups_dict):\n",
    "    return sum(len(idxs) - 1 for idxs in dups_dict.values())\n",
    "\n",
    "def keep_first_indices(hashes):\n",
    "    seen, keep = set(), []\n",
    "    for i, h in enumerate(hashes):\n",
    "        if h not in seen:\n",
    "            seen.add(h)\n",
    "            keep.append(i)\n",
    "    return np.array(keep, dtype=int)\n",
    "\n",
    "def apply_mask(X, y, keep_idx):\n",
    "    mask = np.zeros(len(X), dtype=bool)\n",
    "    mask[keep_idx] = True\n",
    "    return X[mask], y[mask], mask\n",
    "\n",
    "# compute hashes BEFORE cleaning\n",
    "train_h_bef = hashes_for_split(X_train_bef)\n",
    "val_h_bef   = hashes_for_split(X_val_bef)\n",
    "test_h_bef  = hashes_for_split(X_test_bef)\n",
    "\n",
    "dups_train_bef = find_within_split_dups(train_h_bef)\n",
    "dups_val_bef   = find_within_split_dups(val_h_bef)\n",
    "dups_test_bef  = find_within_split_dups(test_h_bef)\n",
    "\n",
    "within_train_bef = count_within_extra(dups_train_bef)\n",
    "within_val_bef   = count_within_extra(dups_val_bef)\n",
    "within_test_bef  = count_within_extra(dups_test_bef)\n",
    "\n",
    "# cross-split overlaps BEFORE cleaning\n",
    "train_set_bef = set(train_h_bef)\n",
    "val_set_bef   = set(val_h_bef)\n",
    "test_set_bef  = set(test_h_bef)\n",
    "\n",
    "val_in_train_bef  = np.where(np.isin(val_h_bef,  list(train_set_bef)))[0]\n",
    "test_in_train_bef = np.where(np.isin(test_h_bef, list(train_set_bef)))[0]\n",
    "train_in_val_bef  = np.where(np.isin(train_h_bef, list(val_set_bef)))[0]\n",
    "train_in_test_bef = np.where(np.isin(train_h_bef, list(test_set_bef)))[0]\n",
    "val_in_test_bef   = np.where(np.isin(val_h_bef,  list(test_set_bef)))[0]\n",
    "test_in_val_bef   = np.where(np.isin(test_h_bef, list(val_set_bef)))[0]\n",
    "\n",
    "print(\"\\n=== DUPLICATE SUMMARY (before cleaning) ===\")\n",
    "print(f\"[TRAIN] within groups: {len(dups_train_bef)} | extras: {within_train_bef}\")\n",
    "print(f\"[VAL]   within groups: {len(dups_val_bef)}   | extras: {within_val_bef}\")\n",
    "print(f\"[TEST]  within groups: {len(dups_test_bef)}  | extras: {within_test_bef}\")\n",
    "print(f\"[VAL]   duplicates in TRAIN: {len(val_in_train_bef)}\")\n",
    "print(f\"[TEST]  duplicates in TRAIN: {len(test_in_train_bef)}\")\n",
    "print(f\"[TRAIN] duplicates in VAL:   {len(train_in_val_bef)}\")\n",
    "print(f\"[TRAIN] duplicates in TEST:  {len(train_in_test_bef)}\")\n",
    "print(f\"[VAL]   duplicates in TEST:  {len(val_in_test_bef)}\")\n",
    "print(f\"[TEST]  duplicates in VAL:   {len(test_in_val_bef)}\")\n",
    "\n",
    "# Remove duplicates (toggle)\n",
    "REMOVE_WITHIN_SPLIT = True\n",
    "REMOVE_VALTEST_IF_IN_TRAIN = True\n",
    "changed = False\n",
    "\n",
    "# 1) within-split dedupe\n",
    "if REMOVE_WITHIN_SPLIT:\n",
    "    keep_tr = keep_first_indices(train_h_bef)\n",
    "    keep_va = keep_first_indices(val_h_bef)\n",
    "    keep_te = keep_first_indices(test_h_bef)\n",
    "\n",
    "    if len(keep_tr) < len(X_train) or len(keep_va) < len(X_val) or len(keep_te) < len(X_test):\n",
    "        X_train, y_train, _ = apply_mask(X_train, y_train, keep_tr)\n",
    "        X_val,   y_val,   _ = apply_mask(X_val,   y_val,   keep_va)\n",
    "        X_test,  y_test,  _ = apply_mask(X_test,  y_test,  keep_te)\n",
    "        changed = True\n",
    "\n",
    "# re-hash after within-split\n",
    "train_hashes = hashes_for_split(X_train)\n",
    "val_hashes   = hashes_for_split(X_val)\n",
    "test_hashes  = hashes_for_split(X_test)\n",
    "train_set    = set(train_hashes)\n",
    "\n",
    "# 2) remove any VAL/TEST samples that appear in TRAIN\n",
    "if REMOVE_VALTEST_IF_IN_TRAIN:\n",
    "    val_keep_idx  = np.where(~np.isin(val_hashes,  list(train_set)))[0]\n",
    "    test_keep_idx = np.where(~np.isin(test_hashes, list(train_set)))[0]\n",
    "    if len(val_keep_idx) < len(X_val) or len(test_keep_idx) < len(X_test):\n",
    "        X_val,  y_val,  _ = apply_mask(X_val,  y_val,  val_keep_idx)\n",
    "        X_test, y_test, _ = apply_mask(X_test, y_test, test_keep_idx)\n",
    "        changed = True\n",
    "\n",
    "print(\"\\n\" + (\"Duplicates removed.\" if changed else \"No duplicates removed.\"))\n",
    "print(f\"Sizes â€” Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# SER model comparison on one dataset (YAMNet vs VGGish)\n",
    "# - Offline augmentation on RAW AUDIO for the whole dataset\n",
    "# - Global audio de-dup BEFORE split (hash on waveform)\n",
    "# - Shared stratified splits\n",
    "# - Post-split duplicate detection/cleaning + graphs\n",
    "# - BiLSTM classifier on top of embeddings\n",
    "# ==================================================\n",
    "import os, random, warnings, logging, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger('absl').setLevel(logging.ERROR)\n",
    "\n",
    "# ---------------------------\n",
    "# Config (EDIT THESE)\n",
    "# ---------------------------\n",
    "SEED = 42\n",
    "DATASET_PATH = r'G:\\498R\\BanglaSER'       # folder with .wav\n",
    "SAVE_PATH    = r'G:\\498R\\exp_outputs'     # where to store npz, figs, results\n",
    "FEATURE_TYPE = \"vggish\"                   # \"yamnet\" or \"vggish\"\n",
    "TARGET_PER_CLASS = 1000                   # augmentation target per class\n",
    "BATCH_SIZE = 32\n",
    "DURATION_SEC = 4.0\n",
    "TARGET_SR = 16000\n",
    "# Optional: different frame counts per model (not required, but tidy)\n",
    "FIXED_FRAMES = 5 if FEATURE_TYPE.lower()==\"vggish\" else 8\n",
    "RUN_DUP_CHECK_PLOTS = True\n",
    "\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset discovery + labels\n",
    "# ---------------------------\n",
    "# Expected filename format:\n",
    "# Mode-StatementType-Emotion-Intensity-Statement-Repetition-Actor.wav\n",
    "EMOTION_MAPPING = {'01':'happy','02':'sad','03':'angry','04':'surprise','05':'neutral'}\n",
    "\n",
    "def create_dataset_info(root_path):\n",
    "    rows = []\n",
    "    for root, _, files in os.walk(root_path):\n",
    "        for f in files:\n",
    "            if f.lower().endswith('.wav'):\n",
    "                parts = f.split('-')\n",
    "                if len(parts) == 7:\n",
    "                    code = parts[2]\n",
    "                    emotion = EMOTION_MAPPING.get(code, 'unknown')\n",
    "                    rows.append({'file_path': os.path.join(root, f),\n",
    "                                 'emotion': emotion,\n",
    "                                 'emotion_code': code})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = create_dataset_info(DATASET_PATH)\n",
    "assert len(df) > 0, \"No .wav files found.\"\n",
    "emotions = sorted(df['emotion'].unique())\n",
    "label_mapping = {lab:i for i,lab in enumerate(emotions)}\n",
    "reverse_mapping = {i:lab for lab,i in label_mapping.items()}\n",
    "num_labels = len(emotions)\n",
    "print(\"Label map:\", label_mapping)\n",
    "\n",
    "# ---------------------------\n",
    "# Audio IO + augmentation\n",
    "# ---------------------------\n",
    "TARGET_LEN = int(TARGET_SR * DURATION_SEC)\n",
    "\n",
    "def load_audio_fixed(path, sr=TARGET_SR, duration=DURATION_SEC):\n",
    "    y, sr = librosa.load(path, sr=sr, duration=duration, mono=True)\n",
    "    if len(y) < TARGET_LEN: y = np.pad(y, (0, TARGET_LEN - len(y)))\n",
    "    else:                   y = y[:TARGET_LEN]\n",
    "    return y.astype(np.float32, copy=False), sr\n",
    "\n",
    "def ensure_length(y, L=TARGET_LEN):\n",
    "    if len(y) < L: y = np.pad(y, (0, L-len(y)))\n",
    "    elif len(y) > L: y = y[:L]\n",
    "    return y.astype(np.float32, copy=False)\n",
    "\n",
    "def aug_noise(y, std=0.005): return (y + np.random.normal(0.0, std, size=y.shape)).astype(np.float32)\n",
    "def aug_shift(y, max_ratio=0.1):\n",
    "    m = int(len(y)*max_ratio); s = np.random.randint(-m, m+1)\n",
    "    if s==0: return y\n",
    "    if s>0:  return np.concatenate([np.zeros(s, y.dtype), y[:-s]]).astype(np.float32)\n",
    "    s=-s;    return np.concatenate([y[s:], np.zeros(s, y.dtype)]).astype(np.float32)\n",
    "def aug_pitch(y, sr=TARGET_SR, rng=(-2,2)):\n",
    "    steps = np.random.uniform(rng[0], rng[1])\n",
    "    z = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=steps)\n",
    "    return ensure_length(z, TARGET_LEN)\n",
    "def aug_stretch(y, rng=(0.8,1.25)):\n",
    "    r = np.random.uniform(rng[0], rng[1])\n",
    "    z = librosa.effects.time_stretch(y=y, rate=r)\n",
    "    return ensure_length(z, TARGET_LEN)\n",
    "\n",
    "AUG_FUNCS = ['noise','shift','pitch','stretch']\n",
    "def apply_aug(y, m):\n",
    "    return aug_noise(y) if m=='noise' else aug_shift(y) if m=='shift' else aug_pitch(y) if m=='pitch' else aug_stretch(y)\n",
    "\n",
    "# ---------------------------\n",
    "# YAMNet/VGGish feature extractors\n",
    "# ---------------------------\n",
    "YAMNET_HANDLE = \"https://tfhub.dev/google/yamnet/1\"\n",
    "VGGISH_HANDLE = \"https://tfhub.dev/google/vggish/1\"\n",
    "\n",
    "print(\"Loading TF-Hub modelsâ€¦\")\n",
    "yamnet_model = hub.load(YAMNET_HANDLE)\n",
    "vggish_model = hub.load(VGGISH_HANDLE)\n",
    "\n",
    "YAMNET_DIM, VGGISH_DIM = 1024, 128\n",
    "\n",
    "def pad_trunc_frames(X, T=FIXED_FRAMES, D=None):\n",
    "    # X: (n, d)\n",
    "    if X is None or X.size == 0:\n",
    "        d = (X.shape[1] if (X is not None and X.ndim==2 and X.size>0) else (D if D is not None else YAMNET_DIM))\n",
    "        return np.zeros((T, d), dtype=np.float32)\n",
    "    n, d = X.shape\n",
    "    if n >= T: return X[:T]\n",
    "    out = np.zeros((T,d), dtype=X.dtype)\n",
    "    out[:n] = X\n",
    "    return out\n",
    "\n",
    "def yamnet_embed_frames(y, sr=TARGET_SR):\n",
    "    if sr != 16000:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=16000)\n",
    "    w = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "    scores, e, _ = yamnet_model(w)          # e: (n,1024)\n",
    "    e = e.numpy().astype(np.float32)\n",
    "    return pad_trunc_frames(e, T=FIXED_FRAMES, D=YAMNET_DIM)\n",
    "\n",
    "def vggish_embed_frames(y, sr=TARGET_SR):\n",
    "    if sr != 16000:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=16000)\n",
    "    w = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "    e = vggish_model(w).numpy().astype(np.float32)  # (n,128)\n",
    "    return pad_trunc_frames(e, T=FIXED_FRAMES, D=VGGISH_DIM)\n",
    "\n",
    "def extract_fn_for(feature_type):\n",
    "    return yamnet_embed_frames if feature_type.lower()==\"yamnet\" else vggish_embed_frames\n",
    "\n",
    "# ==================================================\n",
    "# Global augmentation, then GLOBAL de-dup on RAW AUDIO (hashing)\n",
    "# ==================================================\n",
    "def hash_sample_audio(arr, decimals=6):\n",
    "    \"\"\"Robust hash for 1D float audio arrays.\"\"\"\n",
    "    a = np.asarray(arr, dtype=np.float32)\n",
    "    if a.ndim == 0:\n",
    "        a = a.reshape(1)\n",
    "    a = np.round(a, decimals=decimals)\n",
    "    a = np.ascontiguousarray(a)\n",
    "    return hashlib.sha1(a.tobytes()).hexdigest()\n",
    "\n",
    "def build_augmented_audio_dataset(df, target_per_class=1000):\n",
    "    \"\"\"Return raw audio clips AFTER augmentation + global de-dup (audio-level), with labels.\"\"\"\n",
    "    raw_audio_list = []   # list of 1D float32 arrays (len = TARGET_LEN)\n",
    "    labels_list    = []   # int labels\n",
    "    by_class = defaultdict(list)\n",
    "\n",
    "    # originals\n",
    "    print(\"\\nLoading base audio â€¦\")\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        y, sr = load_audio_fixed(row['file_path'], sr=TARGET_SR, duration=DURATION_SEC)\n",
    "        lab = label_mapping[row['emotion']]\n",
    "        raw_audio_list.append(y.astype(np.float32, copy=False))\n",
    "        labels_list.append(lab)\n",
    "        by_class[lab].append(y.astype(np.float32, copy=False))\n",
    "\n",
    "    # augment to balance\n",
    "    print(\"Augmenting to balance â€¦\")\n",
    "    for lab, auds in by_class.items():\n",
    "        need = max(0, target_per_class - len(auds))\n",
    "        i = 0\n",
    "        while need > 0:\n",
    "            base = auds[i % len(auds)]\n",
    "            m = AUG_FUNCS[np.random.randint(0, len(AUG_FUNCS))]\n",
    "            ya = apply_aug(base, m)\n",
    "            ya = ensure_length(ya, TARGET_LEN).astype(np.float32, copy=False)\n",
    "            raw_audio_list.append(ya)\n",
    "            labels_list.append(lab)\n",
    "            i += 1\n",
    "            need -= 1\n",
    "\n",
    "    labels = np.array(labels_list, dtype=np.int32)\n",
    "\n",
    "    print(\"Global audio de-dup (before split) â€¦\")\n",
    "    hashes = [hash_sample_audio(x) for x in raw_audio_list]\n",
    "    seen, keep_idx = set(), []\n",
    "    for i, h in enumerate(hashes):\n",
    "        if h not in seen:\n",
    "            seen.add(h)\n",
    "            keep_idx.append(i)\n",
    "    keep_idx = np.array(keep_idx, dtype=int)\n",
    "\n",
    "    raw_audio_clean = [raw_audio_list[i] for i in keep_idx]\n",
    "    labels_clean    = labels[keep_idx]\n",
    "\n",
    "    print(f\"Kept {len(raw_audio_clean)} / {len(raw_audio_list)} after global audio de-dup.\")\n",
    "    return raw_audio_clean, labels_clean\n",
    "\n",
    "# ==================================================\n",
    "# Split once (shared by both models)\n",
    "# ==================================================\n",
    "SPLITS_FILE = os.path.join(SAVE_PATH, \"splits_indices_audiohash.npz\")\n",
    "\n",
    "def get_or_make_splits_shared(labels_clean):\n",
    "    if os.path.exists(SPLITS_FILE):\n",
    "        s = np.load(SPLITS_FILE)\n",
    "        return s['train_idx'], s['val_idx'], s['test_idx']\n",
    "    idx = np.arange(len(labels_clean))\n",
    "    tr, tmp, y_tr, y_tmp = train_test_split(idx, labels_clean, test_size=0.2, random_state=SEED, stratify=labels_clean)\n",
    "    va, te, _, _ = train_test_split(tmp, y_tmp, test_size=0.5, random_state=SEED, stratify=y_tmp)\n",
    "    np.savez_compressed(SPLITS_FILE, train_idx=tr, val_idx=va, test_idx=te)\n",
    "    return tr, va, te\n",
    "\n",
    "# ==================================================\n",
    "# Build embeddings for the chosen feature type\n",
    "# ==================================================\n",
    "def audio_to_embeddings(audio_list, feature_type=\"yamnet\"):\n",
    "    fn = extract_fn_for(feature_type)\n",
    "    X = []\n",
    "    for y in tqdm(audio_list, desc=f\"Embedding {feature_type}\"):\n",
    "        X.append(fn(y, TARGET_SR))  # (T,F)\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    return X\n",
    "\n",
    "# ==================================================\n",
    "# Scaling + datasets\n",
    "# ==================================================\n",
    "def scale_and_make_datasets(X_train, X_val, X_test, y_train, y_val, y_test, batch=BATCH_SIZE):\n",
    "    T, F = X_train.shape[1], X_train.shape[2]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train.reshape(-1, F))\n",
    "\n",
    "    def tfm(X):\n",
    "        N = X.shape[0]\n",
    "        return scaler.transform(X.reshape(-1, F)).reshape(N, T, F).astype(np.float32)\n",
    "\n",
    "    Xtr, Xva, Xte = tfm(X_train), tfm(X_val), tfm(X_test)\n",
    "    y_train = y_train.astype(np.int32); y_val = y_val.astype(np.int32); y_test = y_test.astype(np.int32)\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((Xtr, y_train)).shuffle(len(Xtr), seed=SEED).batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds   = tf.data.Dataset.from_tensor_slices((Xva, y_val)).batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds  = tf.data.Dataset.from_tensor_slices((Xte, y_test)).batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    return Xtr, Xva, Xte, train_ds, val_ds, test_ds, scaler\n",
    "\n",
    "# ==================================================\n",
    "# Model + training + eval\n",
    "# ==================================================\n",
    "def build_bilstm_classifier(T, F, C):\n",
    "    inp = layers.Input(shape=(T, F))\n",
    "    x = layers.Masking()(inp)\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(C, activation='softmax')(x)\n",
    "    model = models.Model(inp, out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def callbacks_for(name):\n",
    "    return [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True, verbose=1),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(SAVE_PATH, f'best_{name}.h5'),\n",
    "                                           monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "    ]\n",
    "\n",
    "def train_and_eval(feature_type, X_train, X_val, X_test, y_train, y_val, y_test, train_ds, val_ds, test_ds):\n",
    "    T, F = X_train.shape[1], X_train.shape[2]\n",
    "    name = f\"{feature_type.upper()}_BiLSTM_T{T}_F{F}\"\n",
    "    model = build_bilstm_classifier(T, F, num_labels)\n",
    "\n",
    "    print(f\"\\nTraining {name} â€¦\")\n",
    "    hist = model.fit(train_ds, validation_data=val_ds, epochs=100, callbacks=callbacks_for(name), verbose=1)\n",
    "    model.load_weights(os.path.join(SAVE_PATH, f'best_{name}.h5'))\n",
    "\n",
    "    # Eval\n",
    "    eval_out = model.evaluate(test_ds, return_dict=True, verbose=1)\n",
    "    test_acc = float(eval_out.get('accuracy', np.nan))\n",
    "    y_prob = model.predict(test_ds, verbose=0)\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "    y_true = np.concatenate([b.numpy() for _, b in test_ds], axis=0)\n",
    "\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"Test acc: {test_acc:.4f} | Macro-F1: {macro_f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(\n",
    "        y_true, y_pred, target_names=[reverse_mapping[i] for i in range(num_labels)], digits=4 \n",
    "    ))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=[reverse_mapping[i] for i in range(num_labels)],\n",
    "                yticklabels=[reverse_mapping[i] for i in range(num_labels)])\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(SAVE_PATH, f'{name}_cm.png')); plt.close()\n",
    "\n",
    "    # Curves\n",
    "    h = hist.history\n",
    "    plt.figure(); plt.plot(h['accuracy']); plt.plot(h['val_accuracy']); plt.title(f'{name} Accuracy'); plt.xlabel('epoch'); plt.ylabel('acc'); plt.legend(['train','val']); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_PATH, f'{name}_acc.png')); plt.close()\n",
    "    plt.figure(); plt.plot(h['loss']); plt.plot(h['val_loss']); plt.title(f'{name} Loss'); plt.xlabel('epoch'); plt.ylabel('loss'); plt.legend(['train','val']); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_PATH, f'{name}_loss.png')); plt.close()\n",
    "\n",
    "    # Save row\n",
    "    res_path = os.path.join(SAVE_PATH, \"results.csv\")\n",
    "    row = pd.DataFrame([{\"feature_type\": feature_type, \"T\": T, \"F\": F, \"test_accuracy\": test_acc, \"macro_f1\": macro_f1}])\n",
    "    if os.path.exists(res_path): row.to_csv(res_path, mode='a', header=False, index=False)\n",
    "    else:                        row.to_csv(res_path, index=False)\n",
    "    print(f\"Saved results to {res_path}\")\n",
    "    return test_acc, macro_f1\n",
    "\n",
    "# ==================================================\n",
    "# >>> Build raw audio dataset (augmented) and de-dup globally <<<\n",
    "# ==================================================\n",
    "raw_audio_clean, labels_clean = build_augmented_audio_dataset(df, target_per_class=TARGET_PER_CLASS)\n",
    "\n",
    "# ==================================================\n",
    "# >>> Get or create shared splits on the CLEANED audio dataset <<<\n",
    "# ==================================================\n",
    "train_idx, val_idx, test_idx = get_or_make_splits_shared(labels_clean)\n",
    "\n",
    "# ==================================================\n",
    "# >>> Convert cleaned audio to embeddings for chosen model, then slice splits <<<\n",
    "# ==================================================\n",
    "X_all = audio_to_embeddings([raw_audio_clean[i] for i in range(len(raw_audio_clean))],\n",
    "                            feature_type=FEATURE_TYPE)\n",
    "y_all = labels_clean.copy()\n",
    "\n",
    "X_train, y_train = X_all[train_idx], y_all[train_idx]\n",
    "X_val,   y_val   = X_all[val_idx],   y_all[val_idx]\n",
    "X_test,  y_test  = X_all[test_idx],  y_all[test_idx]\n",
    "\n",
    "# ==================================================\n",
    "# >>> Scale + make datasets (scaler fit on TRAIN only)\n",
    "# ==================================================\n",
    "X_train, X_val, X_test, train_ds, val_ds, test_ds, scaler = scale_and_make_datasets(\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, batch=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# ==================================================\n",
    "# >>> POST-SPLIT DUPLICATE DETECTION / CLEANING + GRAPHS <<<\n",
    "# ==================================================\n",
    "# snapshot \"before\" for graphs\n",
    "X_train_bef, y_train_bef = X_train.copy(), y_train.copy()\n",
    "X_val_bef,   y_val_bef   = X_val.copy(),   y_val.copy()\n",
    "X_test_bef,  y_test_bef  = X_test.copy(),  y_test.copy()\n",
    "\n",
    "def hash_sample(arr, decimals=6):\n",
    "    a = np.ascontiguousarray(np.round(arr, decimals))\n",
    "    return hashlib.sha1(a.view(np.uint8)).hexdigest()\n",
    "\n",
    "def hashes_for_split(X):\n",
    "    return np.array([hash_sample(x) for x in X])\n",
    "\n",
    "def find_within_split_dups(hashes):\n",
    "    buckets = defaultdict(list)\n",
    "    for i, h in enumerate(hashes):\n",
    "        buckets[h].append(i)\n",
    "    return {h: idxs for h, idxs in buckets.items() if len(idxs) > 1}\n",
    "\n",
    "def count_within_extra(dups_dict):\n",
    "    return sum(len(idxs) - 1 for idxs in dups_dict.values())\n",
    "\n",
    "def keep_first_indices(hashes):\n",
    "    seen, keep = set(), []\n",
    "    for i, h in enumerate(hashes):\n",
    "        if h not in seen:\n",
    "            seen.add(h)\n",
    "            keep.append(i)\n",
    "    return np.array(keep, dtype=int)\n",
    "\n",
    "def apply_mask(X, y, keep_idx):\n",
    "    mask = np.zeros(len(X), dtype=bool)\n",
    "    mask[keep_idx] = True\n",
    "    return X[mask], y[mask], mask\n",
    "\n",
    "# compute hashes BEFORE cleaning\n",
    "train_h_bef = hashes_for_split(X_train_bef)\n",
    "val_h_bef   = hashes_for_split(X_val_bef)\n",
    "test_h_bef  = hashes_for_split(X_test_bef)\n",
    "\n",
    "dups_train_bef = find_within_split_dups(train_h_bef)\n",
    "dups_val_bef   = find_within_split_dups(val_h_bef)\n",
    "dups_test_bef  = find_within_split_dups(test_h_bef)\n",
    "\n",
    "within_train_bef = count_within_extra(dups_train_bef)\n",
    "within_val_bef   = count_within_extra(dups_val_bef)\n",
    "within_test_bef  = count_within_extra(dups_test_bef)\n",
    "\n",
    "# cross-split overlaps BEFORE cleaning\n",
    "train_set_bef = set(train_h_bef)\n",
    "val_set_bef   = set(val_h_bef)\n",
    "test_set_bef  = set(test_h_bef)\n",
    "\n",
    "val_in_train_bef  = np.where(np.isin(val_h_bef,  list(train_set_bef)))[0]\n",
    "test_in_train_bef = np.where(np.isin(test_h_bef, list(train_set_bef)))[0]\n",
    "train_in_val_bef  = np.where(np.isin(train_h_bef, list(val_set_bef)))[0]\n",
    "train_in_test_bef = np.where(np.isin(train_h_bef, list(test_set_bef)))[0]\n",
    "val_in_test_bef   = np.where(np.isin(val_h_bef,  list(test_set_bef)))[0]\n",
    "test_in_val_bef   = np.where(np.isin(test_h_bef, list(val_set_bef)))[0]\n",
    "\n",
    "print(\"\\n=== DUPLICATE SUMMARY (before cleaning) ===\")\n",
    "print(f\"[TRAIN] within groups: {len(dups_train_bef)} | extras: {within_train_bef}\")\n",
    "print(f\"[VAL]   within groups: {len(dups_val_bef)}   | extras: {within_val_bef}\")\n",
    "print(f\"[TEST]  within groups: {len(dups_test_bef)}  | extras: {within_test_bef}\")\n",
    "print(f\"[VAL]   duplicates in TRAIN: {len(val_in_train_bef)}\")\n",
    "print(f\"[TEST]  duplicates in TRAIN: {len(test_in_train_bef)}\")\n",
    "print(f\"[TRAIN] duplicates in VAL:   {len(train_in_val_bef)}\")\n",
    "print(f\"[TRAIN] duplicates in TEST:  {len(train_in_test_bef)}\")\n",
    "print(f\"[VAL]   duplicates in TEST:  {len(val_in_test_bef)}\")\n",
    "print(f\"[TEST]  duplicates in VAL:   {len(test_in_val_bef)}\")\n",
    "\n",
    "# Remove duplicates (toggle)\n",
    "REMOVE_WITHIN_SPLIT = True\n",
    "REMOVE_VALTEST_IF_IN_TRAIN = True\n",
    "changed = False\n",
    "\n",
    "# 1) within-split dedupe\n",
    "if REMOVE_WITHIN_SPLIT:\n",
    "    keep_tr = keep_first_indices(train_h_bef)\n",
    "    keep_va = keep_first_indices(val_h_bef)\n",
    "    keep_te = keep_first_indices(test_h_bef)\n",
    "\n",
    "    if len(keep_tr) < len(X_train) or len(keep_va) < len(X_val) or len(keep_te) < len(X_test):\n",
    "        X_train, y_train, _ = apply_mask(X_train, y_train, keep_tr)\n",
    "        X_val,   y_val,   _ = apply_mask(X_val,   y_val,   keep_va)\n",
    "        X_test,  y_test,  _ = apply_mask(X_test,  y_test,  keep_te)\n",
    "        changed = True\n",
    "\n",
    "# re-hash after within-split\n",
    "train_hashes = hashes_for_split(X_train)\n",
    "val_hashes   = hashes_for_split(X_val)\n",
    "test_hashes  = hashes_for_split(X_test)\n",
    "train_set    = set(train_hashes)\n",
    "\n",
    "# 2) remove any VAL/TEST samples that appear in TRAIN\n",
    "if REMOVE_VALTEST_IF_IN_TRAIN:\n",
    "    val_keep_idx  = np.where(~np.isin(val_hashes,  list(train_set)))[0]\n",
    "    test_keep_idx = np.where(~np.isin(test_hashes, list(train_set)))[0]\n",
    "    if len(val_keep_idx) < len(X_val) or len(test_keep_idx) < len(X_test):\n",
    "        X_val,  y_val,  _ = apply_mask(X_val,  y_val,  val_keep_idx)\n",
    "        X_test, y_test, _ = apply_mask(X_test, y_test, test_keep_idx)\n",
    "        changed = True\n",
    "\n",
    "print(\"\\n\" + (\"Duplicates removed.\" if changed else \"No duplicates removed.\"))\n",
    "print(f\"Sizes â€” Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "# --- POST-clean for graphs ---\n",
    "train_h_aft = hashes_for_split(X_train)\n",
    "val_h_aft   = hashes_for_split(X_val)\n",
    "test_h_aft  = hashes_for_split(X_test)\n",
    "\n",
    "dups_train_aft = find_within_split_dups(train_h_aft)\n",
    "dups_val_aft   = find_within_split_dups(val_h_aft)\n",
    "dups_test_aft  = find_within_split_dups(test_h_aft)\n",
    "\n",
    "within_train_aft = count_within_extra(dups_train_aft)\n",
    "within_val_aft   = count_within_extra(dups_val_aft)\n",
    "within_test_aft  = count_within_extra(dups_test_aft)\n",
    "\n",
    "# GRAPHS\n",
    "if RUN_DUP_CHECK_PLOTS:\n",
    "    # 1) Counts before vs after\n",
    "    sizes_before = [len(X_train_bef), len(X_val_bef), len(X_test_bef)]\n",
    "    sizes_after  = [len(X_train),     len(X_val),     len(X_test)]\n",
    "    splits = ['Train', 'Val', 'Test']\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    x = np.arange(len(splits)); w = 0.38\n",
    "    plt.bar(x - w/2, sizes_before, width=w, label='Before')\n",
    "    plt.bar(x + w/2, sizes_after,  width=w, label='After')\n",
    "    plt.xticks(x, splits); plt.ylabel('Num samples')\n",
    "    plt.title('Split sizes: before vs after duplicate cleaning')\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_PATH, 'dup_sizes_before_after.png')); plt.close()\n",
    "\n",
    "    # 2) Within-split duplicates (extras) before vs after\n",
    "    within_bef = [within_train_bef, within_val_bef, within_test_bef]\n",
    "    within_aft = [within_train_aft, within_val_aft, within_test_aft]\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.bar(x - w/2, within_bef, width=w, label='Before')\n",
    "    plt.bar(x + w/2, within_aft, width=w, label='After')\n",
    "    plt.xticks(x, splits); plt.ylabel('Num duplicate extras')\n",
    "    plt.title('Within-split duplicate extras: before vs after')\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_PATH, 'dup_within_extras_before_after.png')); plt.close()\n",
    "\n",
    "    # 3) Cross-split overlap heatmap (BEFORE cleaning)\n",
    "    cross_overlap_before = np.array([\n",
    "        [within_train_bef, len(val_in_train_bef), len(test_in_train_bef)],\n",
    "        [len(train_in_val_bef), within_val_bef, len(val_in_test_bef)],\n",
    "        [len(train_in_test_bef), len(test_in_val_bef), within_test_bef]\n",
    "    ], dtype=int)\n",
    "\n",
    "    plt.figure(figsize=(7,6))\n",
    "    sns.heatmap(cross_overlap_before, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Train','Val','Test'],\n",
    "                yticklabels=['Train','Val','Test'])\n",
    "    plt.title('Duplicate overlap (BEFORE cleaning)\\n(diagonal = within extras)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_PATH, 'dup_overlap_heatmap_before.png')); plt.close()\n",
    "\n",
    "    # 4) Class distribution per split â€” before and after (stacked bars)\n",
    "    def plot_class_dist(y_train, y_val, y_test, title, fname):\n",
    "        classes = [reverse_mapping[i] for i in range(len(reverse_mapping))]\n",
    "        counts_tr = np.array([np.sum(y_train == i) for i in range(len(reverse_mapping))])\n",
    "        counts_va = np.array([np.sum(y_val   == i) for i in range(len(reverse_mapping))])\n",
    "        counts_te = np.array([np.sum(y_test  == i) for i in range(len(reverse_mapping))])\n",
    "\n",
    "        plt.figure(figsize=(10,6))\n",
    "        bottoms = np.zeros(3)\n",
    "        for i, cls in enumerate(classes):\n",
    "            vals = np.array([counts_tr[i], counts_va[i], counts_te[i]])\n",
    "            plt.bar(['Train','Val','Test'], vals, bottom=bottoms, label=cls)\n",
    "            bottoms += vals\n",
    "        plt.ylabel('Num samples'); plt.title(title)\n",
    "        plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "        plt.tight_layout(); plt.savefig(os.path.join(SAVE_PATH, fname)); plt.close()\n",
    "\n",
    "    plot_class_dist(y_train_bef, y_val_bef, y_test_bef,\n",
    "                    'Class distribution per split (BEFORE cleaning)',\n",
    "                    'class_dist_before.png')\n",
    "\n",
    "    plot_class_dist(y_train, y_val, y_test,\n",
    "                    'Class distribution per split (AFTER cleaning)',\n",
    "                    'class_dist_after.png')\n",
    "\n",
    "    print(\"ðŸ“ˆ Saved figures:\",\n",
    "          \"dup_sizes_before_after.png,\",\n",
    "          \"dup_within_extras_before_after.png,\",\n",
    "          \"dup_overlap_heatmap_before.png,\",\n",
    "          \"class_dist_before.png,\",\n",
    "          \"class_dist_after.png\")\n",
    "\n",
    "# ==================================================\n",
    "# >>> Rebuild tf.data datasets after cleaning <<<\n",
    "# ==================================================\n",
    "X_train_cln, X_val_cln, X_test_cln = X_train, X_val, X_test  # already cleaned above\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train_cln, y_train)).shuffle(len(X_train_cln), seed=SEED).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds   = tf.data.Dataset.from_tensor_slices((X_val_cln,   y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds  = tf.data.Dataset.from_tensor_slices((X_test_cln,  y_test)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ==================================================\n",
    "# >>> Train & evaluate\n",
    "# ==================================================\n",
    "acc, f1 = train_and_eval(FEATURE_TYPE, X_train_cln, X_val_cln, X_test_cln, y_train, y_val, y_test,\n",
    "                         train_ds, val_ds, test_ds)\n",
    "\n",
    "print(f\"\\nDone: {FEATURE_TYPE.upper()} | acc={acc:.4f} | macro_f1={f1:.4f}\")\n",
    "print(\"Tip: change FEATURE_TYPE to 'yamnet' (or 'vggish') and run again. Splits are shared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Yamnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# SER model comparison on one dataset (YAMNet vs VGGish)\n",
    "# - Offline augmentation on RAW AUDIO for the whole dataset\n",
    "# - Global audio de-dup BEFORE split (hash on waveform)\n",
    "# - Shared stratified splits\n",
    "# - Post-split duplicate detection/cleaning + graphs\n",
    "# - BiLSTM classifier on top of embeddings\n",
    "# ==================================================\n",
    "import os, random, warnings, logging, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger('absl').setLevel(logging.ERROR)\n",
    "\n",
    "# ---------------------------\n",
    "# Config (EDIT THESE)\n",
    "# ---------------------------\n",
    "SEED = 42\n",
    "DATASET_PATH = r'G:\\498R\\BanglaSER'       # folder with .wav\n",
    "SAVE_PATH    = r'G:\\498R\\exp_outputs'     # where to store npz, figs, results\n",
    "FEATURE_TYPE = \"yamnet\"                   # \"yamnet\" or \"vggish\"\n",
    "TARGET_PER_CLASS = 1000                   # augmentation target per class\n",
    "BATCH_SIZE = 32\n",
    "DURATION_SEC = 4.0\n",
    "TARGET_SR = 16000\n",
    "# Optional: different frame counts per model (not required, but tidy)\n",
    "FIXED_FRAMES = 5 if FEATURE_TYPE.lower()==\"vggish\" else 8\n",
    "RUN_DUP_CHECK_PLOTS = True\n",
    "\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset discovery + labels\n",
    "# ---------------------------\n",
    "# Expected filename format:\n",
    "# Mode-StatementType-Emotion-Intensity-Statement-Repetition-Actor.wav\n",
    "EMOTION_MAPPING = {'01':'happy','02':'sad','03':'angry','04':'surprise','05':'neutral'}\n",
    "\n",
    "def create_dataset_info(root_path):\n",
    "    rows = []\n",
    "    for root, _, files in os.walk(root_path):\n",
    "        for f in files:\n",
    "            if f.lower().endswith('.wav'):\n",
    "                parts = f.split('-')\n",
    "                if len(parts) == 7:\n",
    "                    code = parts[2]\n",
    "                    emotion = EMOTION_MAPPING.get(code, 'unknown')\n",
    "                    rows.append({'file_path': os.path.join(root, f),\n",
    "                                 'emotion': emotion,\n",
    "                                 'emotion_code': code})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = create_dataset_info(DATASET_PATH)\n",
    "assert len(df) > 0, \"No .wav files found.\"\n",
    "emotions = sorted(df['emotion'].unique())\n",
    "label_mapping = {lab:i for i,lab in enumerate(emotions)}\n",
    "reverse_mapping = {i:lab for lab,i in label_mapping.items()}\n",
    "num_labels = len(emotions)\n",
    "print(\"Label map:\", label_mapping)\n",
    "\n",
    "# ---------------------------\n",
    "# Audio IO + augmentation\n",
    "# ---------------------------\n",
    "TARGET_LEN = int(TARGET_SR * DURATION_SEC)\n",
    "\n",
    "def load_audio_fixed(path, sr=TARGET_SR, duration=DURATION_SEC):\n",
    "    y, sr = librosa.load(path, sr=sr, duration=duration, mono=True)\n",
    "    if len(y) < TARGET_LEN: y = np.pad(y, (0, TARGET_LEN - len(y)))\n",
    "    else:                   y = y[:TARGET_LEN]\n",
    "    return y.astype(np.float32, copy=False), sr\n",
    "\n",
    "def ensure_length(y, L=TARGET_LEN):\n",
    "    if len(y) < L: y = np.pad(y, (0, L-len(y)))\n",
    "    elif len(y) > L: y = y[:L]\n",
    "    return y.astype(np.float32, copy=False)\n",
    "\n",
    "def aug_noise(y, std=0.005): return (y + np.random.normal(0.0, std, size=y.shape)).astype(np.float32)\n",
    "def aug_shift(y, max_ratio=0.1):\n",
    "    m = int(len(y)*max_ratio); s = np.random.randint(-m, m+1)\n",
    "    if s==0: return y\n",
    "    if s>0:  return np.concatenate([np.zeros(s, y.dtype), y[:-s]]).astype(np.float32)\n",
    "    s=-s;    return np.concatenate([y[s:], np.zeros(s, y.dtype)]).astype(np.float32)\n",
    "def aug_pitch(y, sr=TARGET_SR, rng=(-2,2)):\n",
    "    steps = np.random.uniform(rng[0], rng[1])\n",
    "    z = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=steps)\n",
    "    return ensure_length(z, TARGET_LEN)\n",
    "def aug_stretch(y, rng=(0.8,1.25)):\n",
    "    r = np.random.uniform(rng[0], rng[1])\n",
    "    z = librosa.effects.time_stretch(y=y, rate=r)\n",
    "    return ensure_length(z, TARGET_LEN)\n",
    "\n",
    "AUG_FUNCS = ['noise','shift','pitch','stretch']\n",
    "def apply_aug(y, m):\n",
    "    return aug_noise(y) if m=='noise' else aug_shift(y) if m=='shift' else aug_pitch(y) if m=='pitch' else aug_stretch(y)\n",
    "\n",
    "# ---------------------------\n",
    "# YAMNet/VGGish feature extractors\n",
    "# ---------------------------\n",
    "YAMNET_HANDLE = \"https://tfhub.dev/google/yamnet/1\"\n",
    "VGGISH_HANDLE = \"https://tfhub.dev/google/vggish/1\"\n",
    "\n",
    "print(\"Loading TF-Hub modelsâ€¦\")\n",
    "yamnet_model = hub.load(YAMNET_HANDLE)\n",
    "vggish_model = hub.load(VGGISH_HANDLE)\n",
    "\n",
    "YAMNET_DIM, VGGISH_DIM = 1024, 128\n",
    "\n",
    "def pad_trunc_frames(X, T=FIXED_FRAMES, D=None):\n",
    "    # X: (n, d)\n",
    "    if X is None or X.size == 0:\n",
    "        d = (X.shape[1] if (X is not None and X.ndim==2 and X.size>0) else (D if D is not None else YAMNET_DIM))\n",
    "        return np.zeros((T, d), dtype=np.float32)\n",
    "    n, d = X.shape\n",
    "    if n >= T: return X[:T]\n",
    "    out = np.zeros((T,d), dtype=X.dtype)\n",
    "    out[:n] = X\n",
    "    return out\n",
    "\n",
    "def yamnet_embed_frames(y, sr=TARGET_SR):\n",
    "    if sr != 16000:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=16000)\n",
    "    w = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "    scores, e, _ = yamnet_model(w)          # e: (n,1024)\n",
    "    e = e.numpy().astype(np.float32)\n",
    "    return pad_trunc_frames(e, T=FIXED_FRAMES, D=YAMNET_DIM)\n",
    "\n",
    "def vggish_embed_frames(y, sr=TARGET_SR):\n",
    "    if sr != 16000:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=16000)\n",
    "    w = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "    e = vggish_model(w).numpy().astype(np.float32)  # (n,128)\n",
    "    return pad_trunc_frames(e, T=FIXED_FRAMES, D=VGGISH_DIM)\n",
    "\n",
    "def extract_fn_for(feature_type):\n",
    "    return yamnet_embed_frames if feature_type.lower()==\"yamnet\" else vggish_embed_frames\n",
    "\n",
    "# ==================================================\n",
    "# Global augmentation, then GLOBAL de-dup on RAW AUDIO (hashing)\n",
    "# ==================================================\n",
    "def hash_sample_audio(arr, decimals=6):\n",
    "    \"\"\"Robust hash for 1D float audio arrays.\"\"\"\n",
    "    a = np.asarray(arr, dtype=np.float32)\n",
    "    if a.ndim == 0:\n",
    "        a = a.reshape(1)\n",
    "    a = np.round(a, decimals=decimals)\n",
    "    a = np.ascontiguousarray(a)\n",
    "    return hashlib.sha1(a.tobytes()).hexdigest()\n",
    "\n",
    "def build_augmented_audio_dataset(df, target_per_class=1000):\n",
    "    \"\"\"Return raw audio clips AFTER augmentation + global de-dup (audio-level), with labels.\"\"\"\n",
    "    raw_audio_list = []   # list of 1D float32 arrays (len = TARGET_LEN)\n",
    "    labels_list    = []   # int labels\n",
    "    by_class = defaultdict(list)\n",
    "\n",
    "    # originals\n",
    "    print(\"\\nLoading base audio â€¦\")\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        y, sr = load_audio_fixed(row['file_path'], sr=TARGET_SR, duration=DURATION_SEC)\n",
    "        lab = label_mapping[row['emotion']]\n",
    "        raw_audio_list.append(y.astype(np.float32, copy=False))\n",
    "        labels_list.append(lab)\n",
    "        by_class[lab].append(y.astype(np.float32, copy=False))\n",
    "\n",
    "    # augment to balance\n",
    "    print(\"Augmenting to balance â€¦\")\n",
    "    for lab, auds in by_class.items():\n",
    "        need = max(0, target_per_class - len(auds))\n",
    "        i = 0\n",
    "        while need > 0:\n",
    "            base = auds[i % len(auds)]\n",
    "            m = AUG_FUNCS[np.random.randint(0, len(AUG_FUNCS))]\n",
    "            ya = apply_aug(base, m)\n",
    "            ya = ensure_length(ya, TARGET_LEN).astype(np.float32, copy=False)\n",
    "            raw_audio_list.append(ya)\n",
    "            labels_list.append(lab)\n",
    "            i += 1\n",
    "            need -= 1\n",
    "\n",
    "    labels = np.array(labels_list, dtype=np.int32)\n",
    "\n",
    "    print(\"Global audio de-dup (before split) â€¦\")\n",
    "    hashes = [hash_sample_audio(x) for x in raw_audio_list]\n",
    "    seen, keep_idx = set(), []\n",
    "    for i, h in enumerate(hashes):\n",
    "        if h not in seen:\n",
    "            seen.add(h)\n",
    "            keep_idx.append(i)\n",
    "    keep_idx = np.array(keep_idx, dtype=int)\n",
    "\n",
    "    raw_audio_clean = [raw_audio_list[i] for i in keep_idx]\n",
    "    labels_clean    = labels[keep_idx]\n",
    "\n",
    "    print(f\"Kept {len(raw_audio_clean)} / {len(raw_audio_list)} after global audio de-dup.\")\n",
    "    return raw_audio_clean, labels_clean\n",
    "\n",
    "# ==================================================\n",
    "# Split once (shared by both models)\n",
    "# ==================================================\n",
    "SPLITS_FILE = os.path.join(SAVE_PATH, \"splits_indices_audiohash.npz\")\n",
    "\n",
    "def get_or_make_splits_shared(labels_clean):\n",
    "    if os.path.exists(SPLITS_FILE):\n",
    "        s = np.load(SPLITS_FILE)\n",
    "        return s['train_idx'], s['val_idx'], s['test_idx']\n",
    "    idx = np.arange(len(labels_clean))\n",
    "    tr, tmp, y_tr, y_tmp = train_test_split(idx, labels_clean, test_size=0.2, random_state=SEED, stratify=labels_clean)\n",
    "    va, te, _, _ = train_test_split(tmp, y_tmp, test_size=0.5, random_state=SEED, stratify=y_tmp)\n",
    "    np.savez_compressed(SPLITS_FILE, train_idx=tr, val_idx=va, test_idx=te)\n",
    "    return tr, va, te\n",
    "\n",
    "# ==================================================\n",
    "# Build embeddings for the chosen feature type\n",
    "# ==================================================\n",
    "def audio_to_embeddings(audio_list, feature_type=\"yamnet\"):\n",
    "    fn = extract_fn_for(feature_type)\n",
    "    X = []\n",
    "    for y in tqdm(audio_list, desc=f\"Embedding {feature_type}\"):\n",
    "        X.append(fn(y, TARGET_SR))  # (T,F)\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    return X\n",
    "\n",
    "# ==================================================\n",
    "# Scaling + datasets\n",
    "# ==================================================\n",
    "def scale_and_make_datasets(X_train, X_val, X_test, y_train, y_val, y_test, batch=BATCH_SIZE):\n",
    "    T, F = X_train.shape[1], X_train.shape[2]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train.reshape(-1, F))\n",
    "\n",
    "    def tfm(X):\n",
    "        N = X.shape[0]\n",
    "        return scaler.transform(X.reshape(-1, F)).reshape(N, T, F).astype(np.float32)\n",
    "\n",
    "    Xtr, Xva, Xte = tfm(X_train), tfm(X_val), tfm(X_test)\n",
    "    y_train = y_train.astype(np.int32); y_val = y_val.astype(np.int32); y_test = y_test.astype(np.int32)\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((Xtr, y_train)).shuffle(len(Xtr), seed=SEED).batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds   = tf.data.Dataset.from_tensor_slices((Xva, y_val)).batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds  = tf.data.Dataset.from_tensor_slices((Xte, y_test)).batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    return Xtr, Xva, Xte, train_ds, val_ds, test_ds, scaler\n",
    "\n",
    "# ==================================================\n",
    "# Model + training + eval\n",
    "# ==================================================\n",
    "def build_bilstm_classifier(T, F, C):\n",
    "    inp = layers.Input(shape=(T, F))\n",
    "    x = layers.Masking()(inp)\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(C, activation='softmax')(x)\n",
    "    model = models.Model(inp, out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def callbacks_for(name):\n",
    "    return [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True, verbose=1),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(SAVE_PATH, f'best_{name}.h5'),\n",
    "                                           monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "    ]\n",
    "\n",
    "def train_and_eval(feature_type, X_train, X_val, X_test, y_train, y_val, y_test, train_ds, val_ds, test_ds):\n",
    "    T, F = X_train.shape[1], X_train.shape[2]\n",
    "    name = f\"{feature_type.upper()}_BiLSTM_T{T}_F{F}\"\n",
    "    model = build_bilstm_classifier(T, F, num_labels)\n",
    "\n",
    "    print(f\"\\nTraining {name} â€¦\")\n",
    "    hist = model.fit(train_ds, validation_data=val_ds, epochs=100, callbacks=callbacks_for(name), verbose=1)\n",
    "    model.load_weights(os.path.join(SAVE_PATH, f'best_{name}.h5'))\n",
    "\n",
    "    # Eval\n",
    "    eval_out = model.evaluate(test_ds, return_dict=True, verbose=1)\n",
    "    test_acc = float(eval_out.get('accuracy', np.nan))\n",
    "    y_prob = model.predict(test_ds, verbose=0)\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "    y_true = np.concatenate([b.numpy() for _, b in test_ds], axis=0)\n",
    "\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"Test acc: {test_acc:.4f} | Macro-F1: {macro_f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(\n",
    "        y_true, y_pred, target_names=[reverse_mapping[i] for i in range(num_labels)], digits=4   # <-- 4 decimal places\n",
    "    ))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=[reverse_mapping[i] for i in range(num_labels)],\n",
    "                yticklabels=[reverse_mapping[i] for i in range(num_labels)])\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(SAVE_PATH, f'{name}_cm.png')); plt.close()\n",
    "\n",
    "    # Curves\n",
    "    h = hist.history\n",
    "    plt.figure(); plt.plot(h['accuracy']); plt.plot(h['val_accuracy']); plt.title(f'{name} Accuracy'); plt.xlabel('epoch'); plt.ylabel('acc'); plt.legend(['train','val']); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_PATH, f'{name}_acc.png')); plt.close()\n",
    "    plt.figure(); plt.plot(h['loss']); plt.plot(h['val_loss']); plt.title(f'{name} Loss'); plt.xlabel('epoch'); plt.ylabel('loss'); plt.legend(['train','val']); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_PATH, f'{name}_loss.png')); plt.close()\n",
    "\n",
    "    # Save row\n",
    "    res_path = os.path.join(SAVE_PATH, \"results.csv\")\n",
    "    row = pd.DataFrame([{\"feature_type\": feature_type, \"T\": T, \"F\": F, \"test_accuracy\": test_acc, \"macro_f1\": macro_f1}])\n",
    "    if os.path.exists(res_path): row.to_csv(res_path, mode='a', header=False, index=False)\n",
    "    else:                        row.to_csv(res_path, index=False)\n",
    "    print(f\"Saved results to {res_path}\")\n",
    "    return test_acc, macro_f1\n",
    "\n",
    "# ==================================================\n",
    "# >>> Build raw audio dataset (augmented) and de-dup globally <<<\n",
    "# ==================================================\n",
    "raw_audio_clean, labels_clean = build_augmented_audio_dataset(df, target_per_class=TARGET_PER_CLASS)\n",
    "\n",
    "# ==================================================\n",
    "# >>> Get or create shared splits on the CLEANED audio dataset <<<\n",
    "# ==================================================\n",
    "train_idx, val_idx, test_idx = get_or_make_splits_shared(labels_clean)\n",
    "\n",
    "# ==================================================\n",
    "# >>> Convert cleaned audio to embeddings for chosen model, then slice splits <<<\n",
    "# ==================================================\n",
    "X_all = audio_to_embeddings([raw_audio_clean[i] for i in range(len(raw_audio_clean))],\n",
    "                            feature_type=FEATURE_TYPE)\n",
    "y_all = labels_clean.copy()\n",
    "\n",
    "X_train, y_train = X_all[train_idx], y_all[train_idx]\n",
    "X_val,   y_val   = X_all[val_idx],   y_all[val_idx]\n",
    "X_test,  y_test  = X_all[test_idx],  y_all[test_idx]\n",
    "\n",
    "# ==================================================\n",
    "# >>> Scale + make datasets (scaler fit on TRAIN only)\n",
    "# ==================================================\n",
    "X_train, X_val, X_test, train_ds, val_ds, test_ds, scaler = scale_and_make_datasets(\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, batch=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# ==================================================\n",
    "# >>> POST-SPLIT DUPLICATE DETECTION / CLEANING + GRAPHS <<<\n",
    "# ==================================================\n",
    "# snapshot \"before\" for graphs\n",
    "X_train_bef, y_train_bef = X_train.copy(), y_train.copy()\n",
    "X_val_bef,   y_val_bef   = X_val.copy(),   y_val.copy()\n",
    "X_test_bef,  y_test_bef  = X_test.copy(),  y_test.copy()\n",
    "\n",
    "def hash_sample(arr, decimals=6):\n",
    "    a = np.ascontiguousarray(np.round(arr, decimals))\n",
    "    return hashlib.sha1(a.view(np.uint8)).hexdigest()\n",
    "\n",
    "def hashes_for_split(X):\n",
    "    return np.array([hash_sample(x) for x in X])\n",
    "\n",
    "def find_within_split_dups(hashes):\n",
    "    buckets = defaultdict(list)\n",
    "    for i, h in enumerate(hashes):\n",
    "        buckets[h].append(i)\n",
    "    return {h: idxs for h, idxs in buckets.items() if len(idxs) > 1}\n",
    "\n",
    "def count_within_extra(dups_dict):\n",
    "    return sum(len(idxs) - 1 for idxs in dups_dict.values())\n",
    "\n",
    "def keep_first_indices(hashes):\n",
    "    seen, keep = set(), []\n",
    "    for i, h in enumerate(hashes):\n",
    "        if h not in seen:\n",
    "            seen.add(h)\n",
    "            keep.append(i)\n",
    "    return np.array(keep, dtype=int)\n",
    "\n",
    "def apply_mask(X, y, keep_idx):\n",
    "    mask = np.zeros(len(X), dtype=bool)\n",
    "    mask[keep_idx] = True\n",
    "    return X[mask], y[mask], mask\n",
    "\n",
    "# compute hashes BEFORE cleaning\n",
    "train_h_bef = hashes_for_split(X_train_bef)\n",
    "val_h_bef   = hashes_for_split(X_val_bef)\n",
    "test_h_bef  = hashes_for_split(X_test_bef)\n",
    "\n",
    "dups_train_bef = find_within_split_dups(train_h_bef)\n",
    "dups_val_bef   = find_within_split_dups(val_h_bef)\n",
    "dups_test_bef  = find_within_split_dups(test_h_bef)\n",
    "\n",
    "within_train_bef = count_within_extra(dups_train_bef)\n",
    "within_val_bef   = count_within_extra(dups_val_bef)\n",
    "within_test_bef  = count_within_extra(dups_test_bef)\n",
    "\n",
    "# cross-split overlaps BEFORE cleaning\n",
    "train_set_bef = set(train_h_bef)\n",
    "val_set_bef   = set(val_h_bef)\n",
    "test_set_bef  = set(test_h_bef)\n",
    "\n",
    "val_in_train_bef  = np.where(np.isin(val_h_bef,  list(train_set_bef)))[0]\n",
    "test_in_train_bef = np.where(np.isin(test_h_bef, list(train_set_bef)))[0]\n",
    "train_in_val_bef  = np.where(np.isin(train_h_bef, list(val_set_bef)))[0]\n",
    "train_in_test_bef = np.where(np.isin(train_h_bef, list(test_set_bef)))[0]\n",
    "val_in_test_bef   = np.where(np.isin(val_h_bef,  list(test_set_bef)))[0]\n",
    "test_in_val_bef   = np.where(np.isin(test_h_bef, list(val_set_bef)))[0]\n",
    "\n",
    "print(\"\\n=== DUPLICATE SUMMARY (before cleaning) ===\")\n",
    "print(f\"[TRAIN] within groups: {len(dups_train_bef)} | extras: {within_train_bef}\")\n",
    "print(f\"[VAL]   within groups: {len(dups_val_bef)}   | extras: {within_val_bef}\")\n",
    "print(f\"[TEST]  within groups: {len(dups_test_bef)}  | extras: {within_test_bef}\")\n",
    "print(f\"[VAL]   duplicates in TRAIN: {len(val_in_train_bef)}\")\n",
    "print(f\"[TEST]  duplicates in TRAIN: {len(test_in_train_bef)}\")\n",
    "print(f\"[TRAIN] duplicates in VAL:   {len(train_in_val_bef)}\")\n",
    "print(f\"[TRAIN] duplicates in TEST:  {len(train_in_test_bef)}\")\n",
    "print(f\"[VAL]   duplicates in TEST:  {len(val_in_test_bef)}\")\n",
    "print(f\"[TEST]  duplicates in VAL:   {len(test_in_val_bef)}\")\n",
    "\n",
    "# Remove duplicates (toggle)\n",
    "REMOVE_WITHIN_SPLIT = True\n",
    "REMOVE_VALTEST_IF_IN_TRAIN = True\n",
    "changed = False\n",
    "\n",
    "# 1) within-split dedupe\n",
    "if REMOVE_WITHIN_SPLIT:\n",
    "    keep_tr = keep_first_indices(train_h_bef)\n",
    "    keep_va = keep_first_indices(val_h_bef)\n",
    "    keep_te = keep_first_indices(test_h_bef)\n",
    "\n",
    "    if len(keep_tr) < len(X_train) or len(keep_va) < len(X_val) or len(keep_te) < len(X_test):\n",
    "        X_train, y_train, _ = apply_mask(X_train, y_train, keep_tr)\n",
    "        X_val,   y_val,   _ = apply_mask(X_val,   y_val,   keep_va)\n",
    "        X_test,  y_test,  _ = apply_mask(X_test,  y_test,  keep_te)\n",
    "        changed = True\n",
    "\n",
    "# re-hash after within-split\n",
    "train_hashes = hashes_for_split(X_train)\n",
    "val_hashes   = hashes_for_split(X_val)\n",
    "test_hashes  = hashes_for_split(X_test)\n",
    "train_set    = set(train_hashes)\n",
    "\n",
    "# 2) remove any VAL/TEST samples that appear in TRAIN\n",
    "if REMOVE_VALTEST_IF_IN_TRAIN:\n",
    "    val_keep_idx  = np.where(~np.isin(val_hashes,  list(train_set)))[0]\n",
    "    test_keep_idx = np.where(~np.isin(test_hashes, list(train_set)))[0]\n",
    "    if len(val_keep_idx) < len(X_val) or len(test_keep_idx) < len(X_test):\n",
    "        X_val,  y_val,  _ = apply_mask(X_val,  y_val,  val_keep_idx)\n",
    "        X_test, y_test, _ = apply_mask(X_test, y_test, test_keep_idx)\n",
    "        changed = True\n",
    "\n",
    "print(\"\\n\" + (\"Duplicates removed.\" if changed else \"No duplicates removed.\"))\n",
    "print(f\"Sizes â€” Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "# --- POST-clean for graphs ---\n",
    "train_h_aft = hashes_for_split(X_train)\n",
    "val_h_aft   = hashes_for_split(X_val)\n",
    "test_h_aft  = hashes_for_split(X_test)\n",
    "\n",
    "dups_train_aft = find_within_split_dups(train_h_aft)\n",
    "dups_val_aft   = find_within_split_dups(val_h_aft)\n",
    "dups_test_aft  = find_within_split_dups(test_h_aft)\n",
    "\n",
    "within_train_aft = count_within_extra(dups_train_aft)\n",
    "within_val_aft   = count_within_extra(dups_val_aft)\n",
    "within_test_aft  = count_within_extra(dups_test_aft)\n",
    "\n",
    "# GRAPHS\n",
    "if RUN_DUP_CHECK_PLOTS:\n",
    "    # 1) Counts before vs after\n",
    "    sizes_before = [len(X_train_bef), len(X_val_bef), len(X_test_bef)]\n",
    "    sizes_after  = [len(X_train),     len(X_val),     len(X_test)]\n",
    "    splits = ['Train', 'Val', 'Test']\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    x = np.arange(len(splits)); w = 0.38\n",
    "    plt.bar(x - w/2, sizes_before, width=w, label='Before')\n",
    "    plt.bar(x + w/2, sizes_after,  width=w, label='After')\n",
    "    plt.xticks(x, splits); plt.ylabel('Num samples')\n",
    "    plt.title('Split sizes: before vs after duplicate cleaning')\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_PATH, 'dup_sizes_before_after.png')); plt.close()\n",
    "\n",
    "    # 2) Within-split duplicates (extras) before vs after\n",
    "    within_bef = [within_train_bef, within_val_bef, within_test_bef]\n",
    "    within_aft = [within_train_aft, within_val_aft, within_test_aft]\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.bar(x - w/2, within_bef, width=w, label='Before')\n",
    "    plt.bar(x + w/2, within_aft, width=w, label='After')\n",
    "    plt.xticks(x, splits); plt.ylabel('Num duplicate extras')\n",
    "    plt.title('Within-split duplicate extras: before vs after')\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_PATH, 'dup_within_extras_before_after.png')); plt.close()\n",
    "\n",
    "    # 3) Cross-split overlap heatmap (BEFORE cleaning)\n",
    "    cross_overlap_before = np.array([\n",
    "        [within_train_bef, len(val_in_train_bef), len(test_in_train_bef)],\n",
    "        [len(train_in_val_bef), within_val_bef, len(val_in_test_bef)],\n",
    "        [len(train_in_test_bef), len(test_in_val_bef), within_test_bef]\n",
    "    ], dtype=int)\n",
    "\n",
    "    plt.figure(figsize=(7,6))\n",
    "    sns.heatmap(cross_overlap_before, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Train','Val','Test'],\n",
    "                yticklabels=['Train','Val','Test'])\n",
    "    plt.title('Duplicate overlap (BEFORE cleaning)\\n(diagonal = within extras)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_PATH, 'dup_overlap_heatmap_before.png')); plt.close()\n",
    "\n",
    "    # 4) Class distribution per split â€” before and after (stacked bars)\n",
    "    def plot_class_dist(y_train, y_val, y_test, title, fname):\n",
    "        classes = [reverse_mapping[i] for i in range(len(reverse_mapping))]\n",
    "        counts_tr = np.array([np.sum(y_train == i) for i in range(len(reverse_mapping))])\n",
    "        counts_va = np.array([np.sum(y_val   == i) for i in range(len(reverse_mapping))])\n",
    "        counts_te = np.array([np.sum(y_test  == i) for i in range(len(reverse_mapping))])\n",
    "\n",
    "        plt.figure(figsize=(10,6))\n",
    "        bottoms = np.zeros(3)\n",
    "        for i, cls in enumerate(classes):\n",
    "            vals = np.array([counts_tr[i], counts_va[i], counts_te[i]])\n",
    "            plt.bar(['Train','Val','Test'], vals, bottom=bottoms, label=cls)\n",
    "            bottoms += vals\n",
    "        plt.ylabel('Num samples'); plt.title(title)\n",
    "        plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "        plt.tight_layout(); plt.savefig(os.path.join(SAVE_PATH, fname)); plt.close()\n",
    "\n",
    "    plot_class_dist(y_train_bef, y_val_bef, y_test_bef,\n",
    "                    'Class distribution per split (BEFORE cleaning)',\n",
    "                    'class_dist_before.png')\n",
    "\n",
    "    plot_class_dist(y_train, y_val, y_test,\n",
    "                    'Class distribution per split (AFTER cleaning)',\n",
    "                    'class_dist_after.png')\n",
    "\n",
    "    print(\"ðŸ“ˆ Saved figures:\",\n",
    "          \"dup_sizes_before_after.png,\",\n",
    "          \"dup_within_extras_before_after.png,\",\n",
    "          \"dup_overlap_heatmap_before.png,\",\n",
    "          \"class_dist_before.png,\",\n",
    "          \"class_dist_after.png\")\n",
    "\n",
    "# ==================================================\n",
    "# >>> Rebuild tf.data datasets after cleaning <<<\n",
    "# ==================================================\n",
    "X_train_cln, X_val_cln, X_test_cln = X_train, X_val, X_test  # already cleaned above\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train_cln, y_train)).shuffle(len(X_train_cln), seed=SEED).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds   = tf.data.Dataset.from_tensor_slices((X_val_cln,   y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds  = tf.data.Dataset.from_tensor_slices((X_test_cln,  y_test)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ==================================================\n",
    "# >>> Train & evaluate\n",
    "# ==================================================\n",
    "acc, f1 = train_and_eval(FEATURE_TYPE, X_train_cln, X_val_cln, X_test_cln, y_train, y_val, y_test,\n",
    "                         train_ds, val_ds, test_ds)\n",
    "\n",
    "print(f\"\\nDone: {FEATURE_TYPE.upper()} | acc={acc:.4f} | macro_f1={f1:.4f}\")\n",
    "print(\"Tip: change FEATURE_TYPE to 'yamnet' (or 'vggish') and run again. Splits are shared.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7353176,
     "sourceId": 11714488,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
